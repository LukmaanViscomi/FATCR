{"cells":[{"cell_type":"markdown","metadata":{"id":"4Bs3NXPfS87N"},"source":["# FACTR â€” Claims Extraction + Embeddings\n","**Version:** v2025-09-07_1.0  \n","**Purpose:** Read UTTERANCES.parquet â†’ extract claims (OpenAI) â†’ write CLAIMS_raw.jsonl â†’ compute embeddings (stub).\n"],"id":"4Bs3NXPfS87N"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H-7GDLeNUT1A","executionInfo":{"status":"ok","timestamp":1758100553216,"user_tz":-60,"elapsed":17896,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"outputId":"5308fb94-d0a8-4855-facc-bf4791021c03"},"id":"H-7GDLeNUT1A","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"0ZzH5nHwS87P"},"execution_count":null,"outputs":[],"source":["# Config\n","BATCH = 25\n","MODEL_CHAT = \"gpt-4o-mini\"\n","MODEL_EMB = \"text-embedding-3-small\"\n","PROMPT_PATH = \"claim_extraction_prompt.txt\"  # optional; else fall back to inline prompt\n"],"id":"0ZzH5nHwS87P"},{"cell_type":"code","source":["from google.colab import userdata\n","\n","api_key = userdata.get(\"OPENAI_API_KEY\")\n","print(\"Loaded?\", bool(api_key))\n","print(\"First 6 chars:\", api_key[:6] if api_key else None)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NYVgpeXS1vPh","executionInfo":{"status":"ok","timestamp":1757863835816,"user_tz":-60,"elapsed":399,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"outputId":"6c84cb75-41b8-4042-99ba-c7c99d6d0744"},"id":"NYVgpeXS1vPh","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded? True\n","First 6 chars: sk-pro\n"]}]},{"cell_type":"code","source":["# --- Robust claims extraction (JSON only, with fallbacks & logging) ---\n","import os, json, time, pandas as pd\n","from openai import OpenAI\n","from google.colab import userdata\n","\n","# ---- Config (use existing vars if already defined) ----\n","MODEL_CHAT   = globals().get(\"MODEL_CHAT\", \"gpt-4o-mini\")  # pick any chat model you have access to\n","BATCH        = globals().get(\"BATCH\", 20)                  # number of utterances per request\n","PROMPT_PATH  = globals().get(\"PROMPT_PATH\", \"prompts/claims_prompt.txt\")\n","UTTS_PARQUET = \"/content/drive/MyDrive/FATCR/data/processed/UTTERANCES.parquet\"\n","\n","# ---- Load utterances ----\n","assert os.path.exists(UTTS_PARQUET), \"Run ASR+Diarize first.\"\n","df = pd.read_parquet(UTTS_PARQUET)\n","print(\"Utterances:\", len(df))\n","\n","# ---- Prompt (force JSON-only) ----\n","if os.path.exists(PROMPT_PATH):\n","    prompt_text = open(PROMPT_PATH, \"r\", encoding=\"utf-8\").read().strip()\n","else:\n","    prompt_text = \"\"\"You are a strict JSON generator.\n","\n","Extract theological claims as a **valid JSON array** only.\n","Each array item MUST be a JSON object with the fields:\n","  \"claim_text\"  (string)\n","  \"type\"        (string: e.g., \"doctrine\", \"ethics\", \"history\", or \"other\")\n","  \"topic\"       (string, brief topic label)\n","  \"stance\"      (string: \"affirm\", \"deny\", \"neutral\")\n","  \"confidence\"  (number 0..1)\n","\n","Rules:\n","- Output **JSON only**, no prose, no markdown, no preamble, no trailing text.\n","- If there are no claims, output [].\n","- Never wrap in code fences.\n","\"\"\"\n","\n","# ---- OpenAI client (from Colab Secrets) ----\n","api_key = userdata.get(\"OPENAI_API_KEY\")\n","if not api_key:\n","    raise SystemExit(\"OPENAI_API_KEY not set. Add it in Colab Secrets and rerun.\")\n","client = OpenAI(api_key=api_key)\n","\n","# ---- Helper: safe JSON parse with logging ----\n","def parse_json_or_log(raw: str, dbg_tag: str) -> list:\n","    raw = (raw or \"\").strip()\n","    if not raw:\n","        print(\"âš ï¸ Empty model response.\")\n","        return []\n","    try:\n","        data = json.loads(raw)\n","        if isinstance(data, list):\n","            return data\n","        else:\n","            print(\"âš ï¸ Model returned non-list JSON. Type =\", type(data).__name__)\n","    except Exception as e:\n","        print(\"âš ï¸ Parse failed:\", e)\n","        print(\"   Raw head (first 300 chars):\\n\", raw[:300])\n","    # Save full raw to snapshots for inspection\n","    os.makedirs(\"snapshots\", exist_ok=True)\n","    dbg_path = f\"snapshots/CLAIMS_DEBUG_{dbg_tag}_{int(time.time())}.txt\"\n","    with open(dbg_path, \"w\", encoding=\"utf-8\") as f:\n","        f.write(raw)\n","    print(\"   Saved raw to:\", dbg_path)\n","    return []\n","\n","# ---- Call model for a batch of texts ----\n","def extract_claims_batch(texts):\n","    \"\"\"\n","    texts: list[str]  (concatenated utterances; we rely on the system prompt to return JSON list)\n","    \"\"\"\n","    resp = client.chat.completions.create(\n","        model=MODEL_CHAT,\n","        messages=[\n","            {\"role\": \"system\", \"content\": prompt_text},\n","            {\"role\": \"user\", \"content\": \"\\n\\n\".join(texts)},\n","        ],\n","        temperature=0.2,\n","    )\n","    raw = resp.choices[0].message.content\n","    # dbg tag includes batch size so you can correlate later\n","    return parse_json_or_log(raw, dbg_tag=f\"b{len(texts)}\")\n","\n","# ---- Drive the batching & write JSONL lines ----\n","out_lines = []\n","for i in range(0, len(df), BATCH):\n","    batch = df.iloc[i:i+BATCH]\n","    # keep speaker label (helps the model separate claims)\n","    texts = [f\"{r.speaker}: {r.text}\" for r in batch.itertuples()]\n","    claims = extract_claims_batch(texts)\n","\n","    if claims:\n","        for c in claims:\n","            out_lines.append(json.dumps({\n","                \"utterance_range\": [int(i), int(i + len(batch) - 1)],\n","                \"claim_text\":  c.get(\"claim_text\", \"\")[:300],\n","                \"type\":        c.get(\"type\", \"other\"),\n","                \"topic\":       c.get(\"topic\", \"other\"),\n","                \"stance\":      c.get(\"stance\", \"neutral\"),\n","                \"confidence\":  float(c.get(\"confidence\", 0)),\n","            }, ensure_ascii=False))\n","\n","# ---- Save JSONL ----\n","OUT_PATH = \"/content/drive/MyDrive/FATCR/data/processed/CLAIMS_raw.jsonl\"\n","os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)\n","\n","with open(OUT_PATH, \"w\", encoding=\"utf-8\") as f:\n","    f.write(\"\\n\".join(out_lines))\n","\n","print(f\"âœ… Wrote {OUT_PATH}:\", len(out_lines), \"items\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z16bYg8V4Xga","executionInfo":{"status":"ok","timestamp":1757871723928,"user_tz":-60,"elapsed":95172,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"outputId":"b34aebf5-c947-4788-f1dc-bdf0ced60aa1"},"id":"Z16bYg8V4Xga","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Utterances: 1242\n","âœ… Wrote /content/drive/MyDrive/FATCR/data/processed/CLAIMS_raw.jsonl: 109 items\n"]}]},{"cell_type":"markdown","source":["batching embeddings will be faster, cheaper, and far less likely to hit rate limits. Drop this single cell into FACTR-04 right after you create CLAIMS_raw.jsonl (or wherever you want to embed), then run it."],"metadata":{"id":"OT-iVySySIl3"},"id":"OT-iVySySIl3"},{"cell_type":"code","source":["# === Batch embeddings for CLAIMS_raw.jsonl (fast & robust) ===\n","import os, json, time, math, numpy as np\n","from datetime import datetime, timezone # Import timezone here\n","from openai import OpenAI\n","from google.colab import userdata\n","\n","# ---- Config ----\n","MODEL_EMB   = \"text-embedding-3-small\"   # or \"text-embedding-3-large\"\n","BATCH_EMB   = 64                         # tune for your quota/rate limits\n","TRUNC_CHARS = 8000                       # hard cap per text to avoid 8192 token issues\n","\n","# Resolve paths (prefer FACTR/processed; fall back to CWD)\n","DATA_DIR = \"/content/drive/MyDrive/FATCR/data/processed\"\n","RAW_PATH = os.path.join(DATA_DIR, \"CLAIMS_raw.jsonl\") if os.path.exists(DATA_DIR) else \"CLAIMS_raw.jsonl\"\n","EMB_NPY  = os.path.join(DATA_DIR, \"CLAIMS_embeddings.npy\") if os.path.exists(DATA_DIR) else \"CLAIMS_embeddings.npy\"\n","META_JSON= os.path.join(DATA_DIR, \"CLAIMS_embeddings.meta.json\") if os.path.exists(DATA_DIR) else \"CLAIMS_embeddings.meta.json\"\n","\n","assert os.path.exists(RAW_PATH), f\"Not found: {RAW_PATH}. Run the claims extraction step first.\"\n","\n","# ---- Load OpenAI key from Colab Secrets ----\n","api_key = userdata.get(\"OPENAI_API_KEY\")\n","assert api_key, \"OPENAI_API_KEY missing in Colab Secrets.\"\n","client = OpenAI(api_key=api_key)\n","\n","# ---- Load claims texts in order ----\n","with open(RAW_PATH, \"r\", encoding=\"utf-8\") as f:\n","    records = [json.loads(line) for line in f if line.strip()]\n","\n","texts = [(rec.get(\"claim_text\") or \"\").strip()[:TRUNC_CHARS] for rec in records]\n","print(f\"Claims to embed: {len(texts)} | model={MODEL_EMB}\")\n","\n","# ---- Helper with retry/backoff ----\n","def embed_batch(batch_texts, max_retries=5, base_sleep=2.0):\n","    for attempt in range(max_retries):\n","        try:\n","            resp = client.embeddings.create(model=MODEL_EMB, input=batch_texts)\n","            return [d.embedding for d in resp.data]\n","        except Exception as e:\n","            wait = base_sleep * (2 ** attempt)\n","            print(f\"âš ï¸  Embed call failed (attempt {attempt+1}/{max_retries}): {e} â†’ sleeping {wait:.1f}s\")\n","            time.sleep(wait)\n","    raise RuntimeError(\"Embedding failed after retries.\")\n","\n","# ---- Run in batches ----\n","all_vecs = []\n","n = len(texts)\n","num_batches = math.ceil(n / BATCH_EMB)\n","\n","t0 = time.time()\n","for bi in range(num_batches):\n","    lo, hi = bi*BATCH_EMB, min((bi+1)*BATCH_EMB, n)\n","    batch = texts[lo:hi]\n","    vecs  = embed_batch(batch)\n","    all_vecs.extend(vecs)\n","    if (bi+1) % 5 == 0 or (bi+1) == num_batches:\n","        elapsed = time.time() - t0\n","        print(f\"â€¦ {hi}/{n} embedded | elapsed {elapsed:.1f}s\")\n","\n","# ---- Save outputs ----\n","arr = np.array(all_vecs, dtype=\"float32\")\n","os.makedirs(os.path.dirname(EMB_NPY) or \".\", exist_ok=True)\n","np.save(EMB_NPY, arr)\n","\n","meta = {\n","    \"ts\": datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n","    \"model\": MODEL_EMB,\n","    \"source\": RAW_PATH,\n","    \"count\": int(arr.shape[0]),\n","    \"dim\": int(arr.shape[1]) if arr.size else 0,\n","    \"batch_size\": BATCH_EMB,\n","    \"trunc_chars\": TRUNC_CHARS,\n","}\n","with open(META_JSON, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(meta, f, indent=2)\n","\n","print(f\"âœ… Saved embeddings â†’ {EMB_NPY}  shape={arr.shape}\")\n","print(f\"ğŸ—‚ï¸  Meta â†’ {META_JSON}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VSBYup1OSFgH","executionInfo":{"status":"ok","timestamp":1757872281603,"user_tz":-60,"elapsed":2420,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"outputId":"4ad46bb4-41ee-401d-95cc-5c97fa6a3871"},"id":"VSBYup1OSFgH","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Claims to embed: 109 | model=text-embedding-3-small\n","â€¦ 109/109 embedded | elapsed 1.9s\n","âœ… Saved embeddings â†’ /content/drive/MyDrive/FATCR/data/processed/CLAIMS_embeddings.npy  shape=(109, 1536)\n","ğŸ—‚ï¸  Meta â†’ /content/drive/MyDrive/FATCR/data/processed/CLAIMS_embeddings.meta.json\n"]}]},{"cell_type":"markdown","source":["## Suggestion: before running, quickly confirm your variables:"],"metadata":{"id":"K1_NPH83Qosc"},"id":"K1_NPH83Qosc"},{"cell_type":"code","metadata":{"id":"aNthDwJVS87Q","executionInfo":{"status":"ok","timestamp":1757873239173,"user_tz":-60,"elapsed":27861,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c22c0d58-e1ea-412c-bdbd-a7daf80a95c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Claims: 108\n","Embeddings shape: (108, 1536)\n","âœ… Saved embeddings â†’ CLAIMS_embeddings.npy\n"]}],"source":["# Embeddings stub (plug FAISS/Chroma later)\n","from openai import OpenAI\n","import json, numpy as np, os\n","from google.colab import userdata # Import userdata\n","\n","assert os.path.exists(\"CLAIMS_raw.jsonl\"), \"Run extraction first.\"\n","# api_key = os.getenv(\"OPENAI_API_KEY\") # Use userdata.get instead\n","api_key = userdata.get(\"OPENAI_API_KEY\")\n","if not api_key:\n","    raise SystemExit(\"OPENAI_API_KEY not set.\")\n","client = OpenAI(api_key=api_key)\n","\n","records = [json.loads(x) for x in open(\"CLAIMS_raw.jsonl\",\"r\",encoding=\"utf-8\").read().splitlines() if x.strip()]\n","texts = [r[\"claim_text\"] for r in records]\n","print(\"Claims:\", len(texts))\n","\n","vecs = []\n","for t in texts:\n","    emb = client.embeddings.create(model=MODEL_EMB, input=t).data[0].embedding\n","    vecs.append(emb)\n","vecs = np.array(vecs, dtype=\"float32\")\n","print(\"Embeddings shape:\", vecs.shape)\n","np.save(\"CLAIMS_embeddings.npy\", vecs)\n","print(\"âœ… Saved embeddings â†’ CLAIMS_embeddings.npy\")"],"id":"aNthDwJVS87Q"},{"cell_type":"code","source":["# === Embeddings with metadata (save to Drive/processed) ===\n","from openai import OpenAI\n","import os, json, time, numpy as np\n","from google.colab import userdata\n","\n","# ---- Config ----\n","DATA_DIR   = \"/content/drive/MyDrive/FATCR/data/processed\"\n","RAW_JSON   = os.path.join(DATA_DIR, \"CLAIMS_raw.jsonl\")\n","EMB_NPY    = os.path.join(DATA_DIR, \"CLAIMS_embeddings.npy\")\n","META_JSON  = os.path.join(DATA_DIR, \"CLAIMS_embeddings.meta.json\")\n","\n","MODEL_EMB  = \"text-embedding-3-small\"   # or \"text-embedding-3-large\"\n","\n","# ---- API key ----\n","api_key = userdata.get(\"OPENAI_API_KEY\")\n","if not api_key:\n","    raise SystemExit(\"OPENAI_API_KEY not set. Add it in Colab Secrets and rerun.\")\n","client = OpenAI(api_key=api_key)\n","\n","# ---- Load claims ----\n","assert os.path.exists(RAW_JSON), f\"Not found: {RAW_JSON}. Run claims extraction first.\"\n","records = [json.loads(x) for x in open(RAW_JSON, \"r\", encoding=\"utf-8\").read().splitlines() if x.strip()]\n","texts   = [r[\"claim_text\"] for r in records]\n","print(\"Claims:\", len(texts))\n","\n","# ---- Embed ----\n","vecs = []\n","for t in texts:\n","    emb = client.embeddings.create(model=MODEL_EMB, input=t).data[0].embedding\n","    vecs.append(emb)\n","\n","arr = np.array(vecs, dtype=\"float32\")\n","os.makedirs(DATA_DIR, exist_ok=True)\n","np.save(EMB_NPY, arr)\n","print(f\"âœ… Saved embeddings â†’ {EMB_NPY} shape={arr.shape}\")\n","\n","# ---- Save metadata ----\n","meta = {\n","    \"ts\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n","    \"model\": MODEL_EMB,\n","    \"source\": RAW_JSON,\n","    \"count\": int(arr.shape[0]),\n","    \"dim\": int(arr.shape[1]) if arr.size > 0 else 0,\n","}\n","with open(META_JSON, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(meta, f, indent=2)\n","\n","print(f\"ğŸ—‚ï¸  Meta â†’ {META_JSON}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6PbJDTmIcF2C","executionInfo":{"status":"ok","timestamp":1757873852450,"user_tz":-60,"elapsed":28236,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"outputId":"f7762704-6d1d-4f31-bdd7-a702d64c0fd4"},"id":"6PbJDTmIcF2C","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Claims: 109\n","âœ… Saved embeddings â†’ /content/drive/MyDrive/FATCR/data/processed/CLAIMS_embeddings.npy shape=(109, 1536)\n","ğŸ—‚ï¸  Meta â†’ /content/drive/MyDrive/FATCR/data/processed/CLAIMS_embeddings.meta.json\n"]}]},{"cell_type":"code","source":["# âœ… Smoke test for FACTR_04\n","import os, json, numpy as np\n","\n","DATA_DIR = \"/content/drive/MyDrive/FATCR/data/processed\"\n","RAW_JSON = os.path.join(DATA_DIR, \"CLAIMS_raw.jsonl\")\n","EMB_NPY  = os.path.join(DATA_DIR, \"CLAIMS_embeddings.npy\")\n","\n","# ---- Checks ----\n","assert os.path.exists(RAW_JSON), \"Missing CLAIMS_raw.jsonl\"\n","lines = [x for x in open(RAW_JSON, \"r\", encoding=\"utf-8\").read().splitlines() if x.strip()]\n","assert len(lines) > 0, \"No claims extracted\"\n","\n","assert os.path.exists(EMB_NPY), \"Missing embeddings file\"\n","arr = np.load(EMB_NPY)\n","\n","assert arr.ndim == 2 and arr.shape[0] == len(lines), \"Embeddings size mismatch\"\n","\n","print(f\"âœ… Claims+Embeddings smoke test passed. {len(lines)} claims, embeddings shape = {arr.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n4_mO8_AdUK9","executionInfo":{"status":"ok","timestamp":1757874050688,"user_tz":-60,"elapsed":47,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"outputId":"66ed41bf-7080-473b-d5f2-7f777eb3a5d1"},"id":"n4_mO8_AdUK9","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Claims+Embeddings smoke test passed. 109 claims, embeddings shape = (109, 1536)\n"]}]},{"cell_type":"markdown","source":["## Snapshot (versions, row count, duration) + pointer JSON"],"metadata":{"id":"tyKdTBw0UCqj"},"id":"tyKdTBw0UCqj"},{"cell_type":"code","source":["# === FACTR_04 Claims+Embeddings Snapshot ===\n","import os, json, time, numpy as np\n","\n","ROOT = \"/content/drive/MyDrive/FATCR\"\n","DATA_DIR = f\"{ROOT}/data/processed\"\n","SNAP_DIR = f\"{ROOT}/snapshots\"\n","CLAIMS_JSON = f\"{DATA_DIR}/CLAIMS_raw.jsonl\"\n","EMB_NPY = f\"{DATA_DIR}/CLAIMS_embeddings.npy\"\n","META_JSON = f\"{DATA_DIR}/CLAIMS_embeddings.meta.json\"\n","PTR_PATH  = f\"{DATA_DIR}/LAST_CLAIMS.json\"\n","\n","# ---- Checks ----\n","assert os.path.exists(CLAIMS_JSON), f\"Missing {CLAIMS_JSON}\"\n","assert os.path.exists(EMB_NPY), f\"Missing {EMB_NPY}\"\n","\n","lines = [x for x in open(CLAIMS_JSON, \"r\", encoding=\"utf-8\").read().splitlines() if x.strip()]\n","arr = np.load(EMB_NPY)\n","\n","assert len(lines) > 0, \"No claims extracted\"\n","assert arr.ndim == 2 and arr.shape[0] == len(lines), \"Embeddings size mismatch\"\n","\n","print(\"âœ… Claims+Embeddings snapshot\")\n","print(\"   Claims     :\", len(lines))\n","print(\"   Embeddings :\", arr.shape)\n","\n","# ---- Save snapshot ----\n","snap = {\n","    \"ts\"    : time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n","    \"claims\": len(lines),\n","    \"embeddings_shape\": arr.shape,\n","    \"raw_json\": os.path.relpath(CLAIMS_JSON, ROOT),\n","    \"embeddings_npy\": os.path.relpath(EMB_NPY, ROOT),\n","    \"meta_json\": os.path.relpath(META_JSON, ROOT),\n","}\n","\n","os.makedirs(SNAP_DIR, exist_ok=True)\n","snap_path = f\"{SNAP_DIR}/CLAIMS_SNAPSHOT_{int(time.time())}.json\"\n","with open(snap_path, \"w\") as f:\n","    json.dump(snap, f, indent=2)\n","print(\"ğŸ“ Saved snapshot ->\", os.path.relpath(snap_path, ROOT))\n","\n","# also write a small pointer JSON for git commits\n","with open(PTR_PATH, \"w\") as f:\n","    json.dump({\n","        \"ts\"    : snap[\"ts\"],\n","        \"claims\": len(lines),\n","        \"shape\" : arr.shape,\n","        \"path\"  : os.path.relpath(CLAIMS_JSON, ROOT),\n","    }, f, indent=2)\n","print(\"ğŸ”— Wrote pointer JSON ->\", os.path.relpath(PTR_PATH, ROOT))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"srhfuRb1eduv","executionInfo":{"status":"ok","timestamp":1757874350363,"user_tz":-60,"elapsed":37,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"outputId":"86ae8505-bfe6-45a7-f858-5e1aa22b3647"},"id":"srhfuRb1eduv","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Claims+Embeddings snapshot\n","   Claims     : 109\n","   Embeddings : (109, 1536)\n","ğŸ“ Saved snapshot -> snapshots/CLAIMS_SNAPSHOT_1757874346.json\n","ğŸ”— Wrote pointer JSON -> data/processed/LAST_CLAIMS.json\n"]}]},{"cell_type":"markdown","source":["## Git push helper (commit notebook + pointer JSON + snapshots)"],"metadata":{"id":"O8FmQb_1UIof"},"id":"O8FmQb_1UIof"},{"cell_type":"code","source":["# === FACTR push (commit notebook + pointer JSON + snapshots + optional tag) ===\n","from google.colab import userdata\n","import urllib.parse, os, subprocess, shlex, time, re\n","\n","# Install ipynbname if not already installed\n","try:\n","    import ipynbname\n","except ImportError:\n","    print(\"Installing ipynbname...\")\n","    !pip install ipynbname -q\n","    import ipynbname\n","\n","\n","ROOT = \"/content/drive/MyDrive/FATCR\"\n","os.chdir(ROOT)\n","\n","# ---- Config ----\n","# Change this string if you want a milestone tag (otherwise leave as \"\")\n","MILESTONE = \"FACTR_04: End-to-end pipeline (Claims+Embeddings working)\"\n","\n","# ---- Show repo status first ----\n","print(\"ğŸ“‚ Repo status:\")\n","!git status -sb\n","\n","# ---- Pull (rebase) to avoid non-fast-forward errors ----\n","print(\"\\nğŸ”„ Pulling (rebase)â€¦\")\n","pat = userdata.get(\"GITHUB_PAT\")\n","assert pat, \"Missing GITHUB_PAT in Colab Secrets.\"\n","enc_pat = urllib.parse.quote(pat, safe=\"\")\n","PULL_URL = f\"https://LukmaanViscomi:{enc_pat}@github.com/LukmaanViscomi/FATCR.git\"\n","!git pull --rebase {PULL_URL} main || true\n","\n","# ---- Stage files ----\n","print(\"\\nâ• Staging filesâ€¦\")\n","# Track notebooks + snapshots + pointer JSONs + standard top-level files\n","!git add notebooks snapshots data/processed/LAST_UTTERANCES.json data/processed/LAST_CLAIMS.json README.md .gitignore 2>/dev/null || true\n","\n","# include the notebook youâ€™re running:\n","nb = ipynbname.path().name  # current .ipynb filename\n","!git add notebooks/{nb} 2>/dev/null || true\n","\n","# ---- Commit only if there are changes ----\n","changed = subprocess.run([\"git\", \"diff\", \"--cached\", \"--quiet\"]).returncode != 0\n","if changed:\n","    msg = f\"FACTR snapshot + pointer update [{int(time.time())}]\"\n","    print(\"\\nâœï¸ Commit:\", msg)\n","    !git commit -m {shlex.quote(msg)}\n","else:\n","    print(\"\\nâ„¹ï¸ Nothing new to commit.\")\n","\n","# ---- Push (inject PAT only for the network call) ----\n","print(\"\\nâ¬†ï¸ Pushing to mainâ€¦\")\n","!git push {PULL_URL} HEAD:main\n","\n","# ---- Optional: Milestone tag ----\n","def make_tag_slug(name: str) -> str:\n","    # keep letters/numbers and . _ - ; replace everything else with -\n","    slug = re.sub(r\"[^A-Za-z0-9._-]+\", \"-\", name.strip())\n","    slug = slug.strip(\".-\")  # trim leading/trailing invalid chars\n","    return slug or \"milestone\"\n","\n","if MILESTONE:\n","    tag = make_tag_slug(MILESTONE)\n","    print(f\"\\nğŸ·ï¸ Creating tag: {tag}\")\n","    subprocess.run([\"git\", \"tag\", \"-f\", tag], check=True)        # lightweight tag at HEAD\n","    subprocess.run([\"git\", \"push\", \"origin\", tag], check=True)   # push to remote\n","    print(\"âœ… Tag pushed:\", tag)\n","\n","print(\"\\nâœ… Push complete.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p7Z0lYB2mzVO","executionInfo":{"status":"ok","timestamp":1757877048278,"user_tz":-60,"elapsed":14370,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"outputId":"0a6b889e-6efc-4d48-fd8c-f0732978f324"},"id":"p7Z0lYB2mzVO","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Installing ipynbname...\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hğŸ“‚ Repo status:\n","## \u001b[32mmain\u001b[m...\u001b[31morigin/main\u001b[m [ahead \u001b[32m3\u001b[m]\n"," \u001b[31mM\u001b[m notebooks/FACTR_04_Claims+Embeddings_v2025-09-07_1.0.ipynb\n","\u001b[31m??\u001b[m data/processed/CLAIMS_embeddings.meta.json\n","\n","ğŸ”„ Pulling (rebase)â€¦\n","error: cannot pull with rebase: You have unstaged changes.\n","error: please commit or stash them.\n","\n","â• Staging filesâ€¦\n","\n","â„¹ï¸ Nothing new to commit.\n","\n","â¬†ï¸ Pushing to mainâ€¦\n","Everything up-to-date\n","\n","ğŸ·ï¸ Creating tag: FACTR_04-End-to-end-pipeline-Claims-Embeddings-working\n","âœ… Tag pushed: FACTR_04-End-to-end-pipeline-Claims-Embeddings-working\n","\n","âœ… Push complete.\n"]}]},{"cell_type":"markdown","source":["## Cell 1 â€” Build & save FAISS index (cosine)"],"metadata":{"id":"I0USRACq-hRR"},"id":"I0USRACq-hRR"},{"cell_type":"code","source":["# # === Build & save FAISS index for CLAIMS embeddings (cosine) ===\n","# # Saves: /content/drive/MyDrive/FATCR/data/processed/CLAIMS.faiss\n","# #        /content/drive/MyDrive/FATCR/data/processed/CLAIMS.index.json  (row metadata)\n","\n","# !pip -q install faiss-cpu\n","\n","# import os, json, numpy as np, faiss, time\n","\n","# ROOT      = \"/content/drive/MyDrive/FATCR\"\n","# DATA_DIR  = f\"{ROOT}/data/processed\"\n","# EMB_NPY   = f\"{DATA_DIR}/CLAIMS_embeddings.npy\"\n","# RAW_JSONL = f\"{DATA_DIR}/CLAIMS_raw.jsonl\"\n","# FAISS_IDX = f\"{DATA_DIR}/CLAIMS.faiss\"\n","# META_JSON = f\"{DATA_DIR}/CLAIMS.index.json\"\n","\n","# assert os.path.exists(EMB_NPY),   f\"Missing embeddings: {EMB_NPY}\"\n","# assert os.path.exists(RAW_JSONL), f\"Missing claims: {RAW_JSONL}\"\n","\n","# # 1) Load vectors\n","# vecs = np.load(EMB_NPY).astype(\"float32\")  # (N, D)\n","# N, D = vecs.shape\n","# print(f\"Embeddings: {vecs.shape}\")\n","\n","# # 2) Cosine similarity via inner product on L2-normalized vectors\n","# faiss.normalize_L2(vecs)\n","# index = faiss.IndexFlatIP(D)   # inner-product (with normalized vectors == cosine)\n","# index.add(vecs)\n","# print(\"Index type:\", type(index).__name__)\n","# print(\"Index size:\", index.ntotal)\n","\n","# # 3) Minimal row metadata (keep it small + easy to join later)\n","# rows = []\n","# with open(RAW_JSONL, \"r\", encoding=\"utf-8\") as f:\n","#     for i, line in enumerate(f):\n","#         obj = json.loads(line)\n","#         rows.append({\n","#             \"row_id\": i,\n","#             \"utterance_range\": obj.get(\"utterance_range\", [None, None]),\n","#             \"claim_text\": obj.get(\"claim_text\", \"\"),\n","#             \"type\": obj.get(\"type\", \"other\"),\n","#             \"topic\": obj.get(\"topic\", \"other\"),\n","#             \"stance\": obj.get(\"stance\", \"neutral\"),\n","#             \"confidence\": float(obj.get(\"confidence\", 0.0)),\n","#         })\n","# assert len(rows) == N, f\"Metadata rows ({len(rows)}) != embeddings ({N})\"\n","\n","# # 4) Save\n","# os.makedirs(DATA_DIR, exist_ok=True)\n","# faiss.write_index(index, FAISS_IDX)\n","# with open(META_JSON, \"w\", encoding=\"utf-8\") as f:\n","#     json.dump({\n","#         \"ts\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n","#         \"index\": os.path.basename(FAISS_IDX),\n","#         \"vectors\": os.path.basename(EMB_NPY),\n","#         \"rows\": N,\n","#         \"dim\": D,\n","#         \"sim\": \"cosine (via IP on L2-normalized vectors)\",\n","#         \"meta_sample\": rows[0] if rows else None,\n","#     }, f, indent=2)\n","\n","# # store a compact, query-time metadata file (row-wise)\n","# META_ROWS_JSON = f\"{DATA_DIR}/CLAIMS.rows.min.json\"\n","# with open(META_ROWS_JSON, \"w\", encoding=\"utf-8\") as f:\n","#     json.dump(rows, f, ensure_ascii=False)\n","\n","# print(\"âœ… Saved FAISS index ->\", FAISS_IDX)\n","# print(\"ğŸ—‚ï¸  Saved index meta  ->\", META_JSON)\n","# print(\"ğŸ—‚ï¸  Saved row meta    ->\", META_ROWS_JSON)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"87sUeANq-jOb","executionInfo":{"status":"ok","timestamp":1757882859874,"user_tz":-60,"elapsed":6960,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"outputId":"60157969-b376-4c7b-e894-603ebaaaa981"},"id":"87sUeANq-jOb","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hEmbeddings: (109, 1536)\n","Index type: IndexFlatIP\n","Index size: 109\n","âœ… Saved FAISS index -> /content/drive/MyDrive/FATCR/data/processed/CLAIMS.faiss\n","ğŸ—‚ï¸  Saved index meta  -> /content/drive/MyDrive/FATCR/data/processed/CLAIMS.index.json\n","ğŸ—‚ï¸  Saved row meta    -> /content/drive/MyDrive/FATCR/data/processed/CLAIMS.rows.min.json\n"]}]},{"cell_type":"markdown","source":["## hereâ€™s a clean, copy-paste full cell you can drop into Cell 1 â€” Build & save FAISS index (cosine). It rebuilds the FAISS index and a consistent CLAIMS.index.json from your saved embeddings, updates LAST_FAISS.json, and has guardrails so mismatches canâ€™t be written again."],"metadata":{"id":"Vkc2n73lKUyk"},"id":"Vkc2n73lKUyk"},{"cell_type":"code","source":["# Cell 1 â€” Build & save FAISS index (cosine)\n","# Rebuild FAISS + id_map from saved embeddings (no re-embedding)\n","\n","# Optional: install faiss-cpu if missing (Colab safety net)\n","try:\n","    import faiss  # noqa\n","except Exception:\n","    !pip -q install faiss-cpu\n","    import faiss\n","\n","import os, json, numpy as np\n","from datetime import datetime, timezone\n","\n","# Resolve ROOT/DATA_DIR (reuse if already defined)\n","try:\n","    ROOT\n","except NameError:\n","    ROOT = \"/content/drive/MyDrive/FATCR\"\n","try:\n","    DATA_DIR\n","except NameError:\n","    DATA_DIR = os.path.join(ROOT, \"data\", \"processed\")\n","\n","EMB_PATH  = os.path.join(DATA_DIR, \"CLAIMS_embeddings.npy\")\n","META_PATH = os.path.join(DATA_DIR, \"CLAIMS_embeddings.meta.json\")\n","FAISS_PATH = os.path.join(DATA_DIR, \"CLAIMS.faiss\")\n","MAP_PATH   = os.path.join(DATA_DIR, \"CLAIMS.index.json\")\n","\n","# 1) Load embeddings + meta\n","emb = np.load(EMB_PATH).astype(\"float32\")\n","with open(META_PATH, \"r\", encoding=\"utf-8\") as f:\n","    meta = json.load(f)\n","\n","print(\"Embeddings:\", emb.shape)\n","\n","# 2) Choose metric and normalise if your pipeline used cosine/IP\n","was_normalized = bool(meta.get(\"normalized\", True))\n","d = emb.shape[1]\n","if was_normalized:\n","    faiss.normalize_L2(emb)\n","    index = faiss.IndexFlatIP(d)   # cosine via inner product on L2-normalised vecs\n","    index_type = \"IndexFlatIP\"\n","else:\n","    index = faiss.IndexFlatL2(d)\n","    index_type = \"IndexFlatL2\"\n","\n","# 3) Build index in order\n","index.add(emb)\n","print(\"Index type:\", index_type)\n","print(\"Index size:\", index.ntotal)\n","\n","# Guardrail before writing\n","assert index.ntotal == emb.shape[0], \"Index count != embedding rows. Refusing to write.\"\n","\n","# 4) Persist index + a fresh identity id_map (faiss_id -> row_index)\n","faiss.write_index(index, FAISS_PATH)\n","id_map = {str(i): i for i in range(index.ntotal)}\n","with open(MAP_PATH, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(id_map, f, ensure_ascii=False, indent=2)\n","\n","print(\"âœ… Saved FAISS index  ->\", FAISS_PATH)\n","print(\"âœ… Saved index map    ->\", MAP_PATH)\n","\n","# 5) Refresh pointer meta (optional but recommended)\n","LAST_FAISS = {\n","    \"time\": datetime.now(timezone.utc).isoformat(),\n","    \"artefacts\": {\n","        \"faiss\": os.path.relpath(FAISS_PATH, ROOT),\n","        \"index_json\": os.path.relpath(MAP_PATH, ROOT),\n","    },\n","    \"dim\": d,\n","    \"normalized\": was_normalized,\n","    \"model\": meta.get(\"model_name\") or meta.get(\"model\"),\n","}\n","with open(os.path.join(DATA_DIR, \"LAST_FAISS.json\"), \"w\", encoding=\"utf-8\") as f:\n","    json.dump(LAST_FAISS, f, ensure_ascii=False, indent=2)\n","print(\"âœ… Updated LAST_FAISS.json\")\n","\n","# Final guardrail\n","assert len(id_map) == index.ntotal, \"id_map size != index size. Refusing to proceed.\"\n","print(\"âœ… Rebuild complete and consistent.\")\n"],"metadata":{"id":"MHpf9JktKXUJ"},"id":"MHpf9JktKXUJ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ğŸ” Cell 2 â€” Query helper (embed query â†’ top-k claims + source)"],"metadata":{"id":"fAfpxigV-l9I"},"id":"fAfpxigV-l9I"},{"cell_type":"code","source":["# === Query FAISS index by text ===\n","# Requires OPENAI_API_KEY in Colab Secrets (already used in this notebook).\n","# Prints top-k claims with claim fields + originating utterance text + timestamps.\n","\n","import os, json, numpy as np, faiss, pandas as pd\n","from openai import OpenAI\n","from google.colab import userdata\n","\n","ROOT      = \"/content/drive/MyDrive/FATCR\"\n","DATA_DIR  = f\"{ROOT}/data/processed\"\n","UTTS_PARQ = f\"{DATA_DIR}/UTTERANCES.parquet\"\n","FAISS_IDX = f\"{DATA_DIR}/CLAIMS.faiss\"\n","ROWS_MIN  = f\"{DATA_DIR}/CLAIMS.rows.min.json\"\n","\n","MODEL_EMB = \"text-embedding-3-small\"  # keep consistent with build step\n","TOP_K     = 5\n","\n","# Load artifacts\n","assert os.path.exists(FAISS_IDX), f\"FAISS idx missing: {FAISS_IDX}\"\n","assert os.path.exists(ROWS_MIN),  f\"Row meta missing: {ROWS_MIN}\"\n","assert os.path.exists(UTTS_PARQ), f\"Utterances parquet missing: {UTTS_PARQ}\"\n","\n","index   = faiss.read_index(FAISS_IDX)\n","rows    = json.load(open(ROWS_MIN, \"r\", encoding=\"utf-8\"))\n","df_utts = pd.read_parquet(UTTS_PARQ)\n","\n","# OpenAI client\n","api_key = userdata.get(\"OPENAI_API_KEY\")\n","assert api_key, \"OPENAI_API_KEY missing in Colab Secrets.\"\n","client = OpenAI(api_key=api_key)\n","\n","def embed_text(text: str) -> np.ndarray:\n","    resp = client.embeddings.create(model=MODEL_EMB, input=text)\n","    vec = np.array(resp.data[0].embedding, dtype=\"float32\")[None, :]  # (1, D)\n","    faiss.normalize_L2(vec)\n","    return vec\n","\n","def pretty_sec(s):\n","    try:\n","        return f\"{float(s):.2f}s\"\n","    except Exception:\n","        return str(s)\n","\n","def search_claims(query: str, k: int = TOP_K):\n","    q = embed_text(query)\n","    sims, idxs = index.search(q, k)  # cosine scores because vectors are normalized\n","    sims = sims[0]; idxs = idxs[0]\n","\n","    results = []\n","    for score, ridx in zip(sims, idxs):\n","        meta = rows[int(ridx)]\n","        u0, u1 = meta.get(\"utterance_range\", [None, None])\n","\n","        # fetch source utterance text & timestamps (grab u0; optionally span to u1)\n","        src_txt   = None\n","        t_start   = None\n","        t_end     = None\n","        video_id  = None\n","\n","        try:\n","            row0 = df_utts.iloc[int(u0)]\n","            src_txt  = row0.get(\"text\", \"\")\n","            t_start  = row0.get(\"t_start\", None)\n","            t_end    = row0.get(\"t_end\", None)\n","            video_id = row0.get(\"video_id\", None)\n","        except Exception:\n","            pass\n","\n","        results.append({\n","            \"score\": float(score),\n","            \"row_id\": int(ridx),\n","            \"claim_text\": meta.get(\"claim_text\", \"\"),\n","            \"type\": meta.get(\"type\", \"other\"),\n","            \"topic\": meta.get(\"topic\", \"other\"),\n","            \"stance\": meta.get(\"stance\", \"neutral\"),\n","            \"confidence\": meta.get(\"confidence\", 0.0),\n","            \"utterance_range\": meta.get(\"utterance_range\", [None, None]),\n","            \"source_text\": src_txt,\n","            \"t_start\": t_start,\n","            \"t_end\": t_end,\n","            \"video_id\": video_id,\n","        })\n","    return results\n","\n","def show(results):\n","    for i, r in enumerate(results, 1):\n","        print(f\"\\n#{i}  score={r['score']:.3f}  row={r['row_id']}  conf={r['confidence']:.2f}\")\n","        print(\"    claim :\", r[\"claim_text\"])\n","        print(f\"    meta  : type={r['type']} | topic={r['topic']} | stance={r['stance']}\")\n","        print(f\"    src   : {pretty_sec(r['t_start'])}â€“{pretty_sec(r['t_end'])} | video={r['video_id']}\")\n","        if r[\"source_text\"]:\n","            print(\"    utt   :\", r[\"source_text\"][:220].replace(\"\\n\", \" \"))\n","        print(\"    range :\", r[\"utterance_range\"])\n","\n","# Example:\n","q = \"atonement or salvation by faith\"\n","print(\"Query:\", q)\n","res = search_claims(q, k=5)\n","show(res)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F3Lg9C4Z-p39","executionInfo":{"status":"ok","timestamp":1757882880768,"user_tz":-60,"elapsed":1582,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"outputId":"e384607c-d43c-498e-ec4b-9fc9c42ca037"},"id":"F3Lg9C4Z-p39","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Query: atonement or salvation by faith\n","\n","#1  score=0.471  row=31  conf=0.90\n","    claim : He voluntarily gave himself as the atonement.\n","    meta  : type=doctrine | topic=atonement | stance=affirm\n","    src   : 188.28sâ€“189.88s | video=speFWRuuJNs_16k_mono.wav\n","    utt   : Thank you, was the cup taken away?\n","    range : [150, 174]\n","\n","#2  score=0.424  row=2  conf=0.90\n","    claim : Jesus voluntarily gave himself as the atonement for us.\n","    meta  : type=doctrine | topic=atonement | stance=affirm\n","    src   : 0.00sâ€“2.28s | video=speFWRuuJNs_16k_mono.wav\n","    utt   : part of a pagan practice in part of history.\n","    range : [0, 24]\n","\n","#3  score=0.341  row=1  conf=0.70\n","    claim : Just because we believe in the human sacrifice does not mean that we are pagan.\n","    meta  : type=doctrine | topic=human sacrifice | stance=affirm\n","    src   : 0.00sâ€“2.28s | video=speFWRuuJNs_16k_mono.wav\n","    utt   : part of a pagan practice in part of history.\n","    range : [0, 24]\n","\n","#4  score=0.319  row=29  conf=0.80\n","    claim : Human prayers can be rejected.\n","    meta  : type=doctrine | topic=prayer | stance=affirm\n","    src   : 188.28sâ€“189.88s | video=speFWRuuJNs_16k_mono.wav\n","    utt   : Thank you, was the cup taken away?\n","    range : [150, 174]\n","\n","#5  score=0.315  row=107  conf=0.80\n","    claim : Immortal essence can only belong to God.\n","    meta  : type=doctrine | topic=immortality | stance=affirm\n","    src   : 1687.72sâ€“1688.88s | video=speFWRuuJNs_16k_mono.wav\n","    utt   : In our sense...\n","    range : [1175, 1199]\n"]}]},{"cell_type":"markdown","source":["drop-in FAISS snapshot cell that matches the style of your 02/03/04 snapshots.\n","## It validates the index, logs key stats, and writes both a timestamped snapshot and a tiny pointer JSON you can commit."],"metadata":{"id":"XL2eeOSgDrkD"},"id":"XL2eeOSgDrkD"},{"cell_type":"code","source":["# === FACTR_FAISS Snapshot (index health + pointer) ===\n","# Logs key stats about the FAISS index & related files, and writes:\n","#  - snapshots/FAISS_SNAPSHOT_*.json\n","#  - data/processed/LAST_FAISS.json  (tiny pointer you can commit)\n","\n","import os, json, time, platform\n","import numpy as np\n","import faiss\n","\n","ROOT      = \"/content/drive/MyDrive/FATCR\"\n","DATA_DIR  = f\"{ROOT}/data/processed\"\n","SNAP_DIR  = f\"{ROOT}/snapshots\"\n","\n","FAISS_IDX = f\"{DATA_DIR}/CLAIMS.faiss\"\n","IDX_META  = f\"{DATA_DIR}/CLAIMS.index.json\"     # written in your FAISS build cell\n","ROWS_MIN  = f\"{DATA_DIR}/CLAIMS.rows.min.json\"  # compact row metadata (one per claim)\n","EMB_NPY   = f\"{DATA_DIR}/CLAIMS_embeddings.npy\" # optional: for extra consistency check\n","PTR_PATH  = f\"{DATA_DIR}/LAST_FAISS.json\"       # tiny pointer JSON\n","\n","# --- Basic existence checks\n","for p in [FAISS_IDX, IDX_META, ROWS_MIN]:\n","    assert os.path.exists(p), f\"Missing required file: {p}\"\n","\n","# --- Load artifacts\n","index = faiss.read_index(FAISS_IDX)\n","with open(IDX_META, \"r\", encoding=\"utf-8\") as f:\n","    idx_meta = json.load(f)\n","with open(ROWS_MIN, \"r\", encoding=\"utf-8\") as f:\n","    rows = json.load(f)\n","\n","# Try loading embeddings for an optional extra check (not required)\n","emb_arr = None\n","if os.path.exists(EMB_NPY):\n","    try:\n","        emb_arr = np.load(EMB_NPY, mmap_mode=\"r\")\n","    except Exception:\n","        emb_arr = None  # ignore if load fails; not critical\n","\n","# --- Gather stats / health checks\n","n_index = index.ntotal\n","dim     = index.d\n","\n","n_rows  = len(rows)\n","ok_rows = (n_rows == n_index)\n","\n","emb_shape = None\n","ok_emb    = True\n","if emb_arr is not None:\n","    emb_shape = tuple(emb_arr.shape)\n","    ok_emb = (emb_arr.shape[0] == n_index and emb_arr.shape[1] == dim)\n","\n","# File sizes (human-ish)\n","def _size_mb(p):\n","    return round(os.path.getsize(p) / (1024*1024), 2)\n","\n","sz_faiss   = _size_mb(FAISS_IDX)\n","sz_rows    = _size_mb(ROWS_MIN)\n","sz_meta    = _size_mb(IDX_META)\n","\n","# Try to grab faiss version (not always present)\n","try:\n","    faiss_ver = faiss.__version__\n","except Exception:\n","    faiss_ver = \"unknown\"\n","\n","# --- Print a short summary\n","print(\"âœ… FAISS snapshot\")\n","print(\"  index file :\", os.path.relpath(FAISS_IDX, ROOT), f\"({sz_faiss} MB)\")\n","print(\"  rows file  :\", os.path.relpath(ROWS_MIN, ROOT),  f\"({sz_rows} MB)\")\n","print(\"  meta file  :\", os.path.relpath(IDX_META, ROOT),   f\"({sz_meta} MB)\")\n","print(\"  vectors    :\", n_index)\n","print(\"  dim        :\", dim)\n","print(\"  rows match :\", ok_rows, f\"(rows={n_rows} vs index={n_index})\")\n","if emb_shape:\n","    print(\"  emb shape  :\", emb_shape, \"match:\", ok_emb)\n","\n","# --- Compose snapshot record\n","snap = {\n","    \"ts\"       : time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n","    \"python\"   : platform.python_version(),\n","    \"numpy\"    : np.__version__,\n","    \"faiss\"    : faiss_ver,\n","    \"index\"    : os.path.relpath(FAISS_IDX, ROOT),\n","    \"rows_min\" : os.path.relpath(ROWS_MIN, ROOT),\n","    \"idx_meta\" : os.path.relpath(IDX_META, ROOT),\n","    \"vectors\"  : int(n_index),\n","    \"dim\"      : int(dim),\n","    \"rows_ok\"  : bool(ok_rows),\n","    \"emb_ok\"   : bool(ok_emb),\n","    \"sizes_mb\" : {\n","        \"faiss\" : sz_faiss,\n","        \"rows\"  : sz_rows,\n","        \"meta\"  : sz_meta,\n","    },\n","}\n","\n","# --- Save snapshot + pointer\n","os.makedirs(SNAP_DIR, exist_ok=True)\n","snap_path = f\"{SNAP_DIR}/FAISS_SNAPSHOT_{int(time.time())}.json\"\n","with open(snap_path, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(snap, f, indent=2)\n","print(\"ğŸ“ Saved snapshot ->\", os.path.relpath(snap_path, ROOT))\n","\n","os.makedirs(os.path.dirname(PTR_PATH), exist_ok=True)\n","with open(PTR_PATH, \"w\", encoding=\"utf-8\") as f:\n","    json.dump({\n","        \"ts\"      : snap[\"ts\"],\n","        \"index\"   : snap[\"index\"],\n","        \"rows\"    : snap[\"rows_min\"],\n","        \"vectors\" : snap[\"vectors\"],\n","        \"dim\"     : snap[\"dim\"],\n","    }, f, indent=2)\n","print(\"ğŸ”— Wrote pointer JSON ->\", os.path.relpath(PTR_PATH, ROOT))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3VXUATPLDye-","executionInfo":{"status":"ok","timestamp":1757884190287,"user_tz":-60,"elapsed":59,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"outputId":"3202aef2-d2e7-4bb3-f885-9ec2144e77cc"},"id":"3VXUATPLDye-","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… FAISS snapshot\n","  index file : data/processed/CLAIMS.faiss (0.64 MB)\n","  rows file  : data/processed/CLAIMS.rows.min.json (0.02 MB)\n","  meta file  : data/processed/CLAIMS.index.json (0.0 MB)\n","  vectors    : 109\n","  dim        : 1536\n","  rows match : True (rows=109 vs index=109)\n","  emb shape  : (109, 1536) match: True\n","ğŸ“ Saved snapshot -> snapshots/FAISS_SNAPSHOT_1757884186.json\n","ğŸ”— Wrote pointer JSON -> data/processed/LAST_FAISS.json\n"]}]},{"cell_type":"markdown","source":["## Unified push helper (works for 04 and FAISS)\n","Paste this once (in any notebook); it will stage notebooks, snapshots, pointer JSONs, and FAISS artifacts. It also supports an optional milestone tag."],"metadata":{"id":"4oNhJlg0F333"},"id":"4oNhJlg0F333"},{"cell_type":"code","source":["# === FACTR universal push (commit notebook + snapshots + pointers + FAISS + optional tag) ===\n","from google.colab import userdata\n","import urllib.parse, os, subprocess, shlex, time, re\n","\n","ROOT = \"/content/drive/MyDrive/FATCR\"\n","os.chdir(ROOT)\n","\n","# --- Optional milestone tag: set to \"\" to skip tagging ---\n","MILESTONE = \"FAISS: initial cosine index working 2025-09-14\"  # or e.g. \"FACTR_04: end-to-end 2025-09-14\" / \"\"\n","\n","print(\"ğŸ“‚ Repo status:\")\n","!git status -sb\n","\n","# --- Pull first (rebase) ---\n","print(\"\\nğŸ”„ Pulling (rebase)â€¦\")\n","pat = userdata.get(\"GITHUB_PAT\")\n","assert pat, \"Missing GITHUB_PAT in Colab Secrets.\"\n","enc_pat = urllib.parse.quote(pat, safe=\"\")\n","PULL_URL = f\"https://LukmaanViscomi:{enc_pat}@github.com/LukmaanViscomi/FATCR.git\"\n","!git pull --rebase {PULL_URL} main || true\n","\n","# --- Stage everything we might create across 04+05 (missing files are fine) ---\n","print(\"\\nâ• Staging filesâ€¦\")\n","!git add notebooks snapshots \\\n","  data/processed/LAST_UTTERANCES.json \\\n","  data/processed/LAST_CLAIMS.json \\\n","  data/processed/UTTERANCES.parquet \\\n","  data/processed/CLAIMS_raw.jsonl \\\n","  data/processed/CLAIMS_embeddings.npy \\\n","  data/processed/CLAIMS_embeddings.meta.json \\\n","  data/processed/CLAIMS.faiss \\\n","  data/processed/CLAIMS.index.json \\\n","  data/processed/CLAIMS.rows.min.json \\\n","  data/processed/LAST_FAISS.json \\\n","  README.md .gitignore 2>/dev/null || true\n","\n","# --- (Optional) also stage the notebook you're running, if ipynbname is present ---\n","try:\n","    import ipynbname, sys\n","    nb = ipynbname.path().name\n","    os.system(f\"git add notebooks/{nb} 2>/dev/null || true\")\n","except Exception:\n","    pass\n","\n","# --- Commit if needed ---\n","changed = subprocess.run([\"git\", \"diff\", \"--cached\", \"--quiet\"]).returncode != 0\n","if changed:\n","    msg = f\"FACTR: snapshot/index/pointers update [{int(time.time())}]\"\n","    print(\"\\nâœï¸ Commit:\", msg)\n","    !git commit -m {shlex.quote(msg)}\n","else:\n","    print(\"\\nâ„¹ï¸ Nothing new to commit.\")\n","\n","# --- Push commit ---\n","print(\"\\nâ¬†ï¸ Pushing to mainâ€¦\")\n","!git push {PULL_URL} HEAD:main\n","\n","# --- Optional: Milestone tag ---\n","def make_tag_slug(name: str) -> str:\n","    slug = re.sub(r\"[^A-Za-z0-9._-]\", \"-\", name.strip()).strip(\"-_.\")\n","    return slug or \"milestone\"\n","\n","if MILESTONE:\n","    tag = make_tag_slug(MILESTONE)\n","    print(f\"\\nğŸ·ï¸ Creating tag: {tag}\")\n","    subprocess.run([\"git\", \"tag\", \"-f\", tag], check=True)\n","    subprocess.run([\"git\", \"push\", \"origin\", tag, \"--force\"], check=True)\n","    print(f\"âœ… Tag pushed: {tag}\")\n","\n","print(\"\\nâœ… Push complete.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0N_trTG7F0vl","executionInfo":{"status":"ok","timestamp":1757885094484,"user_tz":-60,"elapsed":5449,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"outputId":"704b8cb3-c364-4708-b52d-daf2fc1de14d"},"id":"0N_trTG7F0vl","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ“‚ Repo status:\n","## \u001b[32mmain\u001b[m...\u001b[31morigin/main\u001b[m [ahead \u001b[32m3\u001b[m]\n","\u001b[32mA\u001b[m  notebooks/FACTR_01_Setup_v2025-09-09_V2.0.ipynb\n","\u001b[32mD\u001b[m  notebooks/FACTR_01_Setup_v2025-09-09_test.ipynb\n","\u001b[32mM\u001b[m  notebooks/FACTR_02_Ingest_v2025-09-07_2.0.ipynb\n","\u001b[32mA\u001b[m  notebooks/FACTR_03_ASR+Diarize_v2025-09-07_2.0.ipynb\n","\u001b[32mD\u001b[m  notebooks/FACTR_04_Claims+Embeddings_v2025-09-07_1.0.ipynb\n","\u001b[32mA\u001b[m  notebooks/FACTR_04_Claims+Embeddings_v2025-09-07_2.0.ipynb\n","\u001b[32mR\u001b[m  notebooks/FACTR_03_ASR+Diarize_v2025-09-07_1.0.ipynb -> notebooks/old/FACTR_03_ASR+Diarize_v2025-09-07_1.0.ipynb\n","\u001b[31m??\u001b[m data/processed/CLAIMS.faiss\n","\u001b[31m??\u001b[m data/processed/CLAIMS.index.json\n","\u001b[31m??\u001b[m data/processed/CLAIMS.rows.min.json\n","\u001b[31m??\u001b[m data/processed/CLAIMS_embeddings.meta.json\n","\u001b[31m??\u001b[m data/processed/LAST_FAISS.json\n","\n","ğŸ”„ Pulling (rebase)â€¦\n","error: cannot pull with rebase: Your index contains uncommitted changes.\n","error: please commit or stash them.\n","\n","â• Staging filesâ€¦\n","\n","âœï¸ Commit: FACTR: snapshot/index/pointers update [1757885086]\n","[main eaec101] FACTR: snapshot/index/pointers update [1757885086]\n"," 7 files changed, 4 insertions(+), 3 deletions(-)\n"," create mode 100644 notebooks/FACTR_01_Setup_v2025-09-09_V2.0.ipynb\n"," delete mode 100644 notebooks/FACTR_01_Setup_v2025-09-09_test.ipynb\n"," rewrite notebooks/FACTR_02_Ingest_v2025-09-07_2.0.ipynb (99%)\n"," create mode 100644 notebooks/FACTR_03_ASR+Diarize_v2025-09-07_2.0.ipynb\n"," delete mode 100644 notebooks/FACTR_04_Claims+Embeddings_v2025-09-07_1.0.ipynb\n"," create mode 100644 notebooks/FACTR_04_Claims+Embeddings_v2025-09-07_2.0.ipynb\n"," rename notebooks/{ => old}/FACTR_03_ASR+Diarize_v2025-09-07_1.0.ipynb (100%)\n","\n","â¬†ï¸ Pushing to mainâ€¦\n","Enumerating objects: 11, done.\n","Counting objects: 100% (11/11), done.\n","Delta compression using up to 2 threads\n","Compressing objects: 100% (8/8), done.\n","Writing objects: 100% (8/8), 32.07 KiB | 729.00 KiB/s, done.\n","Total 8 (delta 1), reused 0 (delta 0), pack-reused 0\n","remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n","To https://github.com/LukmaanViscomi/FATCR.git\n","   3055777..eaec101  HEAD -> main\n","\n","ğŸ·ï¸ Creating tag: FAISS--initial-cosine-index-working-2025-09-14\n","âœ… Tag pushed: FAISS--initial-cosine-index-working-2025-09-14\n","\n","âœ… Push complete.\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","pygments_lexer":"ipython3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}