{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# FACTR \u2014 Claims Extraction + Embeddings\n**Version:** v2025-09-07_1.0  \n**Purpose:** Read UTTERANCES.parquet \u2192 extract claims (OpenAI) \u2192 write CLAIMS_raw.jsonl \u2192 compute embeddings (stub).\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Config\nBATCH = 25\nMODEL_CHAT = \"gpt-4o-mini\"\nMODEL_EMB = \"text-embedding-3-small\"\nPROMPT_PATH = \"claim_extraction_prompt.txt\"  # optional; else fall back to inline prompt\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import os, json, pandas as pd\nfrom openai import OpenAI\n\nassert os.path.exists(\"UTTERANCES.parquet\"), \"Run ASR+Diarize first.\"\ndf = pd.read_parquet(\"UTTERANCES.parquet\")\nprint(\"Utterances:\", len(df))\n\n# Load prompt\nif os.path.exists(PROMPT_PATH):\n    prompt_text = open(PROMPT_PATH, \"r\", encoding=\"utf-8\").read()\nelse:\n    prompt_text = (\n        \"Extract theological claims as JSON list with fields: \"\n        \"claim_text, type, topic, stance, confidence (0-1). If none, return [].\"\n    )\n\n# OpenAI\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise SystemExit(\"OPENAI_API_KEY not set. Add it to Colab/Env and rerun.\")\nclient = OpenAI(api_key=api_key)\n\ndef extract_claims_batch(texts):\n    resp = client.chat.completions.create(\n        model=MODEL_CHAT,\n        messages=[{\"role\":\"system\",\"content\":prompt_text},\n                  {\"role\":\"user\",\"content\":\"\\n\\n\".join(texts)}],\n        temperature=0.2,\n    )\n    try:\n        data = json.loads(resp.choices[0].message.content)\n        if isinstance(data, list):\n            return data\n    except Exception:\n        return []\n    return []\n\nout_lines = []\nfor i in range(0, len(df), BATCH):\n    batch = df.iloc[i:i+BATCH]\n    texts = [f\"{r.speaker}: {r.text}\" for r in batch.itertuples()]\n    claims = extract_claims_batch(texts)\n    if claims:\n        for c in claims:\n            out_lines.append(json.dumps({\n                \"utterance_range\":[int(i), int(i+len(batch)-1)],\n                \"claim_text\": c.get(\"claim_text\",\"\")[:300],\n                \"type\": c.get(\"type\",\"other\"),\n                \"topic\": c.get(\"topic\",\"other\"),\n                \"stance\": c.get(\"stance\",\"neutral\"),\n                \"confidence\": float(c.get(\"confidence\",0)),\n            }, ensure_ascii=False))\n\nwith open(\"CLAIMS_raw.jsonl\",\"w\",encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(out_lines))\nprint(\"\u2705 Wrote CLAIMS_raw.jsonl:\", len(out_lines), \"items\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Embeddings stub (plug FAISS/Chroma later)\nfrom openai import OpenAI\nimport json, numpy as np, os\n\nassert os.path.exists(\"CLAIMS_raw.jsonl\"), \"Run extraction first.\"\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise SystemExit(\"OPENAI_API_KEY not set.\")\nclient = OpenAI(api_key=api_key)\n\nrecords = [json.loads(x) for x in open(\"CLAIMS_raw.jsonl\",\"r\",encoding=\"utf-8\").read().splitlines() if x.strip()]\ntexts = [r[\"claim_text\"] for r in records]\nprint(\"Claims:\", len(texts))\n\nvecs = []\nfor t in texts:\n    emb = client.embeddings.create(model=MODEL_EMB, input=t).data[0].embedding\n    vecs.append(emb)\nvecs = np.array(vecs, dtype=\"float32\")\nprint(\"Embeddings shape:\", vecs.shape)\nnp.save(\"CLAIMS_embeddings.npy\", vecs)\nprint(\"\u2705 Saved embeddings \u2192 CLAIMS_embeddings.npy\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Smoke test\nimport os, json, numpy as np\nassert os.path.exists(\"CLAIMS_raw.jsonl\"), \"Missing CLAIMS_raw.jsonl\"\nlines = [x for x in open(\"CLAIMS_raw.jsonl\",\"r\",encoding=\"utf-8\").read().splitlines() if x.strip()]\nassert len(lines) > 0, \"No claims extracted\"\nassert os.path.exists(\"CLAIMS_embeddings.npy\"), \"Missing embeddings file\"\narr = np.load(\"CLAIMS_embeddings.npy\")\nassert arr.ndim == 2 and arr.shape[0] == len(lines), \"Embeddings size mismatch\"\nprint(\"\u2705 Claims+Embeddings smoke test passed.\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Snapshot\nimport json, time, os, subprocess\nsnap = {\n  \"ts\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n  \"claims_lines\": sum(1 for _ in open(\"CLAIMS_raw.jsonl\",\"r\",encoding=\"utf-8\")) if os.path.exists(\"CLAIMS_raw.jsonl\") else 0,\n  \"emb_file\": os.path.exists(\"CLAIMS_embeddings.npy\"),\n  \"pip_freeze\": subprocess.check_output([\"pip\",\"freeze\"], text=True).splitlines()[:150],\n}\nos.makedirs(\"snapshots\", exist_ok=True)\nimport time as _t\np = f\"snapshots/CLAIMS_EMB_SNAPSHOT_{int(_t.time())}.json\"\nwith open(p,\"w\") as f: json.dump(snap,f,indent=2)\nprint(\"\ud83d\udcf8 Saved:\", p)\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}