{"cells":[{"cell_type":"markdown","metadata":{"id":"4Bs3NXPfS87N"},"source":["# FACTR ‚Äî Claims Extraction + Embeddings\n","**Version:** v2025-09-07_1.0  \n","**Purpose:** Read UTTERANCES.parquet ‚Üí extract claims (OpenAI) ‚Üí write CLAIMS_raw.jsonl ‚Üí compute embeddings (stub).\n"],"id":"4Bs3NXPfS87N"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H-7GDLeNUT1A","executionInfo":{"status":"ok","timestamp":1757862535101,"user_tz":-60,"elapsed":883,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"outputId":"10397dae-1fc5-4580-f9d6-fe7ef30b248f"},"id":"H-7GDLeNUT1A","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"0ZzH5nHwS87P","executionInfo":{"status":"ok","timestamp":1757862535107,"user_tz":-60,"elapsed":3,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}}},"execution_count":7,"outputs":[],"source":["# Config\n","BATCH = 25\n","MODEL_CHAT = \"gpt-4o-mini\"\n","MODEL_EMB = \"text-embedding-3-small\"\n","PROMPT_PATH = \"claim_extraction_prompt.txt\"  # optional; else fall back to inline prompt\n"],"id":"0ZzH5nHwS87P"},{"cell_type":"code","source":["from google.colab import userdata\n","\n","api_key = userdata.get(\"OPENAI_API_KEY\")\n","print(\"Loaded?\", bool(api_key))\n","print(\"First 6 chars:\", api_key[:6] if api_key else None)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NYVgpeXS1vPh","executionInfo":{"status":"ok","timestamp":1757863835816,"user_tz":-60,"elapsed":399,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"outputId":"6c84cb75-41b8-4042-99ba-c7c99d6d0744"},"id":"NYVgpeXS1vPh","execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded? True\n","First 6 chars: sk-pro\n"]}]},{"cell_type":"code","source":["# --- Robust claims extraction (JSON only, with fallbacks & logging) ---\n","import os, json, time, pandas as pd\n","from openai import OpenAI\n","from google.colab import userdata\n","\n","# ---- Config (use existing vars if already defined) ----\n","MODEL_CHAT   = globals().get(\"MODEL_CHAT\", \"gpt-4o-mini\")  # pick any chat model you have access to\n","BATCH        = globals().get(\"BATCH\", 20)                  # number of utterances per request\n","PROMPT_PATH  = globals().get(\"PROMPT_PATH\", \"prompts/claims_prompt.txt\")\n","UTTS_PARQUET = \"/content/drive/MyDrive/FATCR/data/processed/UTTERANCES.parquet\"\n","\n","# ---- Load utterances ----\n","assert os.path.exists(UTTS_PARQUET), \"Run ASR+Diarize first.\"\n","df = pd.read_parquet(UTTS_PARQUET)\n","print(\"Utterances:\", len(df))\n","\n","# ---- Prompt (force JSON-only) ----\n","if os.path.exists(PROMPT_PATH):\n","    prompt_text = open(PROMPT_PATH, \"r\", encoding=\"utf-8\").read().strip()\n","else:\n","    prompt_text = \"\"\"You are a strict JSON generator.\n","\n","Extract theological claims as a **valid JSON array** only.\n","Each array item MUST be a JSON object with the fields:\n","  \"claim_text\"  (string)\n","  \"type\"        (string: e.g., \"doctrine\", \"ethics\", \"history\", or \"other\")\n","  \"topic\"       (string, brief topic label)\n","  \"stance\"      (string: \"affirm\", \"deny\", \"neutral\")\n","  \"confidence\"  (number 0..1)\n","\n","Rules:\n","- Output **JSON only**, no prose, no markdown, no preamble, no trailing text.\n","- If there are no claims, output [].\n","- Never wrap in code fences.\n","\"\"\"\n","\n","# ---- OpenAI client (from Colab Secrets) ----\n","api_key = userdata.get(\"OPENAI_API_KEY\")\n","if not api_key:\n","    raise SystemExit(\"OPENAI_API_KEY not set. Add it in Colab Secrets and rerun.\")\n","client = OpenAI(api_key=api_key)\n","\n","# ---- Helper: safe JSON parse with logging ----\n","def parse_json_or_log(raw: str, dbg_tag: str) -> list:\n","    raw = (raw or \"\").strip()\n","    if not raw:\n","        print(\"‚ö†Ô∏è Empty model response.\")\n","        return []\n","    try:\n","        data = json.loads(raw)\n","        if isinstance(data, list):\n","            return data\n","        else:\n","            print(\"‚ö†Ô∏è Model returned non-list JSON. Type =\", type(data).__name__)\n","    except Exception as e:\n","        print(\"‚ö†Ô∏è Parse failed:\", e)\n","        print(\"   Raw head (first 300 chars):\\n\", raw[:300])\n","    # Save full raw to snapshots for inspection\n","    os.makedirs(\"snapshots\", exist_ok=True)\n","    dbg_path = f\"snapshots/CLAIMS_DEBUG_{dbg_tag}_{int(time.time())}.txt\"\n","    with open(dbg_path, \"w\", encoding=\"utf-8\") as f:\n","        f.write(raw)\n","    print(\"   Saved raw to:\", dbg_path)\n","    return []\n","\n","# ---- Call model for a batch of texts ----\n","def extract_claims_batch(texts):\n","    \"\"\"\n","    texts: list[str]  (concatenated utterances; we rely on the system prompt to return JSON list)\n","    \"\"\"\n","    resp = client.chat.completions.create(\n","        model=MODEL_CHAT,\n","        messages=[\n","            {\"role\": \"system\", \"content\": prompt_text},\n","            {\"role\": \"user\", \"content\": \"\\n\\n\".join(texts)},\n","        ],\n","        temperature=0.2,\n","    )\n","    raw = resp.choices[0].message.content\n","    # dbg tag includes batch size so you can correlate later\n","    return parse_json_or_log(raw, dbg_tag=f\"b{len(texts)}\")\n","\n","# ---- Drive the batching & write JSONL lines ----\n","out_lines = []\n","for i in range(0, len(df), BATCH):\n","    batch = df.iloc[i:i+BATCH]\n","    # keep speaker label (helps the model separate claims)\n","    texts = [f\"{r.speaker}: {r.text}\" for r in batch.itertuples()]\n","    claims = extract_claims_batch(texts)\n","\n","    if claims:\n","        for c in claims:\n","            out_lines.append(json.dumps({\n","                \"utterance_range\": [int(i), int(i + len(batch) - 1)],\n","                \"claim_text\":  c.get(\"claim_text\", \"\")[:300],\n","                \"type\":        c.get(\"type\", \"other\"),\n","                \"topic\":       c.get(\"topic\", \"other\"),\n","                \"stance\":      c.get(\"stance\", \"neutral\"),\n","                \"confidence\":  float(c.get(\"confidence\", 0)),\n","            }, ensure_ascii=False))\n","\n","# ---- Save JSONL ----\n","OUT_PATH = \"/content/drive/MyDrive/FATCR/data/processed/CLAIMS_raw.jsonl\"\n","os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)\n","\n","with open(OUT_PATH, \"w\", encoding=\"utf-8\") as f:\n","    f.write(\"\\n\".join(out_lines))\n","\n","print(f\"‚úÖ Wrote {OUT_PATH}:\", len(out_lines), \"items\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z16bYg8V4Xga","executionInfo":{"status":"ok","timestamp":1757871723928,"user_tz":-60,"elapsed":95172,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"outputId":"b34aebf5-c947-4788-f1dc-bdf0ced60aa1"},"id":"Z16bYg8V4Xga","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Utterances: 1242\n","‚úÖ Wrote /content/drive/MyDrive/FATCR/data/processed/CLAIMS_raw.jsonl: 109 items\n"]}]},{"cell_type":"markdown","source":["batching embeddings will be faster, cheaper, and far less likely to hit rate limits. Drop this single cell into FACTR-04 right after you create CLAIMS_raw.jsonl (or wherever you want to embed), then run it."],"metadata":{"id":"OT-iVySySIl3"},"id":"OT-iVySySIl3"},{"cell_type":"code","source":["# === Batch embeddings for CLAIMS_raw.jsonl (fast & robust) ===\n","import os, json, time, math, numpy as np\n","from datetime import datetime, timezone # Import timezone here\n","from openai import OpenAI\n","from google.colab import userdata\n","\n","# ---- Config ----\n","MODEL_EMB   = \"text-embedding-3-small\"   # or \"text-embedding-3-large\"\n","BATCH_EMB   = 64                         # tune for your quota/rate limits\n","TRUNC_CHARS = 8000                       # hard cap per text to avoid 8192 token issues\n","\n","# Resolve paths (prefer FACTR/processed; fall back to CWD)\n","DATA_DIR = \"/content/drive/MyDrive/FATCR/data/processed\"\n","RAW_PATH = os.path.join(DATA_DIR, \"CLAIMS_raw.jsonl\") if os.path.exists(DATA_DIR) else \"CLAIMS_raw.jsonl\"\n","EMB_NPY  = os.path.join(DATA_DIR, \"CLAIMS_embeddings.npy\") if os.path.exists(DATA_DIR) else \"CLAIMS_embeddings.npy\"\n","META_JSON= os.path.join(DATA_DIR, \"CLAIMS_embeddings.meta.json\") if os.path.exists(DATA_DIR) else \"CLAIMS_embeddings.meta.json\"\n","\n","assert os.path.exists(RAW_PATH), f\"Not found: {RAW_PATH}. Run the claims extraction step first.\"\n","\n","# ---- Load OpenAI key from Colab Secrets ----\n","api_key = userdata.get(\"OPENAI_API_KEY\")\n","assert api_key, \"OPENAI_API_KEY missing in Colab Secrets.\"\n","client = OpenAI(api_key=api_key)\n","\n","# ---- Load claims texts in order ----\n","with open(RAW_PATH, \"r\", encoding=\"utf-8\") as f:\n","    records = [json.loads(line) for line in f if line.strip()]\n","\n","texts = [(rec.get(\"claim_text\") or \"\").strip()[:TRUNC_CHARS] for rec in records]\n","print(f\"Claims to embed: {len(texts)} | model={MODEL_EMB}\")\n","\n","# ---- Helper with retry/backoff ----\n","def embed_batch(batch_texts, max_retries=5, base_sleep=2.0):\n","    for attempt in range(max_retries):\n","        try:\n","            resp = client.embeddings.create(model=MODEL_EMB, input=batch_texts)\n","            return [d.embedding for d in resp.data]\n","        except Exception as e:\n","            wait = base_sleep * (2 ** attempt)\n","            print(f\"‚ö†Ô∏è  Embed call failed (attempt {attempt+1}/{max_retries}): {e} ‚Üí sleeping {wait:.1f}s\")\n","            time.sleep(wait)\n","    raise RuntimeError(\"Embedding failed after retries.\")\n","\n","# ---- Run in batches ----\n","all_vecs = []\n","n = len(texts)\n","num_batches = math.ceil(n / BATCH_EMB)\n","\n","t0 = time.time()\n","for bi in range(num_batches):\n","    lo, hi = bi*BATCH_EMB, min((bi+1)*BATCH_EMB, n)\n","    batch = texts[lo:hi]\n","    vecs  = embed_batch(batch)\n","    all_vecs.extend(vecs)\n","    if (bi+1) % 5 == 0 or (bi+1) == num_batches:\n","        elapsed = time.time() - t0\n","        print(f\"‚Ä¶ {hi}/{n} embedded | elapsed {elapsed:.1f}s\")\n","\n","# ---- Save outputs ----\n","arr = np.array(all_vecs, dtype=\"float32\")\n","os.makedirs(os.path.dirname(EMB_NPY) or \".\", exist_ok=True)\n","np.save(EMB_NPY, arr)\n","\n","meta = {\n","    \"ts\": datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n","    \"model\": MODEL_EMB,\n","    \"source\": RAW_PATH,\n","    \"count\": int(arr.shape[0]),\n","    \"dim\": int(arr.shape[1]) if arr.size else 0,\n","    \"batch_size\": BATCH_EMB,\n","    \"trunc_chars\": TRUNC_CHARS,\n","}\n","with open(META_JSON, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(meta, f, indent=2)\n","\n","print(f\"‚úÖ Saved embeddings ‚Üí {EMB_NPY}  shape={arr.shape}\")\n","print(f\"üóÇÔ∏è  Meta ‚Üí {META_JSON}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VSBYup1OSFgH","executionInfo":{"status":"ok","timestamp":1757872281603,"user_tz":-60,"elapsed":2420,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"outputId":"4ad46bb4-41ee-401d-95cc-5c97fa6a3871"},"id":"VSBYup1OSFgH","execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Claims to embed: 109 | model=text-embedding-3-small\n","‚Ä¶ 109/109 embedded | elapsed 1.9s\n","‚úÖ Saved embeddings ‚Üí /content/drive/MyDrive/FATCR/data/processed/CLAIMS_embeddings.npy  shape=(109, 1536)\n","üóÇÔ∏è  Meta ‚Üí /content/drive/MyDrive/FATCR/data/processed/CLAIMS_embeddings.meta.json\n"]}]},{"cell_type":"markdown","source":["## Suggestion: before running, quickly confirm your variables:"],"metadata":{"id":"K1_NPH83Qosc"},"id":"K1_NPH83Qosc"},{"cell_type":"code","metadata":{"id":"aNthDwJVS87Q","executionInfo":{"status":"ok","timestamp":1757873239173,"user_tz":-60,"elapsed":27861,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c22c0d58-e1ea-412c-bdbd-a7daf80a95c7"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Claims: 108\n","Embeddings shape: (108, 1536)\n","‚úÖ Saved embeddings ‚Üí CLAIMS_embeddings.npy\n"]}],"source":["# Embeddings stub (plug FAISS/Chroma later)\n","from openai import OpenAI\n","import json, numpy as np, os\n","from google.colab import userdata # Import userdata\n","\n","assert os.path.exists(\"CLAIMS_raw.jsonl\"), \"Run extraction first.\"\n","# api_key = os.getenv(\"OPENAI_API_KEY\") # Use userdata.get instead\n","api_key = userdata.get(\"OPENAI_API_KEY\")\n","if not api_key:\n","    raise SystemExit(\"OPENAI_API_KEY not set.\")\n","client = OpenAI(api_key=api_key)\n","\n","records = [json.loads(x) for x in open(\"CLAIMS_raw.jsonl\",\"r\",encoding=\"utf-8\").read().splitlines() if x.strip()]\n","texts = [r[\"claim_text\"] for r in records]\n","print(\"Claims:\", len(texts))\n","\n","vecs = []\n","for t in texts:\n","    emb = client.embeddings.create(model=MODEL_EMB, input=t).data[0].embedding\n","    vecs.append(emb)\n","vecs = np.array(vecs, dtype=\"float32\")\n","print(\"Embeddings shape:\", vecs.shape)\n","np.save(\"CLAIMS_embeddings.npy\", vecs)\n","print(\"‚úÖ Saved embeddings ‚Üí CLAIMS_embeddings.npy\")"],"id":"aNthDwJVS87Q"},{"cell_type":"code","source":["# === Embeddings with metadata (save to Drive/processed) ===\n","from openai import OpenAI\n","import os, json, time, numpy as np\n","from google.colab import userdata\n","\n","# ---- Config ----\n","DATA_DIR   = \"/content/drive/MyDrive/FATCR/data/processed\"\n","RAW_JSON   = os.path.join(DATA_DIR, \"CLAIMS_raw.jsonl\")\n","EMB_NPY    = os.path.join(DATA_DIR, \"CLAIMS_embeddings.npy\")\n","META_JSON  = os.path.join(DATA_DIR, \"CLAIMS_embeddings.meta.json\")\n","\n","MODEL_EMB  = \"text-embedding-3-small\"   # or \"text-embedding-3-large\"\n","\n","# ---- API key ----\n","api_key = userdata.get(\"OPENAI_API_KEY\")\n","if not api_key:\n","    raise SystemExit(\"OPENAI_API_KEY not set. Add it in Colab Secrets and rerun.\")\n","client = OpenAI(api_key=api_key)\n","\n","# ---- Load claims ----\n","assert os.path.exists(RAW_JSON), f\"Not found: {RAW_JSON}. Run claims extraction first.\"\n","records = [json.loads(x) for x in open(RAW_JSON, \"r\", encoding=\"utf-8\").read().splitlines() if x.strip()]\n","texts   = [r[\"claim_text\"] for r in records]\n","print(\"Claims:\", len(texts))\n","\n","# ---- Embed ----\n","vecs = []\n","for t in texts:\n","    emb = client.embeddings.create(model=MODEL_EMB, input=t).data[0].embedding\n","    vecs.append(emb)\n","\n","arr = np.array(vecs, dtype=\"float32\")\n","os.makedirs(DATA_DIR, exist_ok=True)\n","np.save(EMB_NPY, arr)\n","print(f\"‚úÖ Saved embeddings ‚Üí {EMB_NPY} shape={arr.shape}\")\n","\n","# ---- Save metadata ----\n","meta = {\n","    \"ts\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n","    \"model\": MODEL_EMB,\n","    \"source\": RAW_JSON,\n","    \"count\": int(arr.shape[0]),\n","    \"dim\": int(arr.shape[1]) if arr.size > 0 else 0,\n","}\n","with open(META_JSON, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(meta, f, indent=2)\n","\n","print(f\"üóÇÔ∏è  Meta ‚Üí {META_JSON}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6PbJDTmIcF2C","executionInfo":{"status":"ok","timestamp":1757873852450,"user_tz":-60,"elapsed":28236,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"outputId":"f7762704-6d1d-4f31-bdd7-a702d64c0fd4"},"id":"6PbJDTmIcF2C","execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Claims: 109\n","‚úÖ Saved embeddings ‚Üí /content/drive/MyDrive/FATCR/data/processed/CLAIMS_embeddings.npy shape=(109, 1536)\n","üóÇÔ∏è  Meta ‚Üí /content/drive/MyDrive/FATCR/data/processed/CLAIMS_embeddings.meta.json\n"]}]},{"cell_type":"code","source":["# ‚úÖ Smoke test for FACTR_04\n","import os, json, numpy as np\n","\n","DATA_DIR = \"/content/drive/MyDrive/FATCR/data/processed\"\n","RAW_JSON = os.path.join(DATA_DIR, \"CLAIMS_raw.jsonl\")\n","EMB_NPY  = os.path.join(DATA_DIR, \"CLAIMS_embeddings.npy\")\n","\n","# ---- Checks ----\n","assert os.path.exists(RAW_JSON), \"Missing CLAIMS_raw.jsonl\"\n","lines = [x for x in open(RAW_JSON, \"r\", encoding=\"utf-8\").read().splitlines() if x.strip()]\n","assert len(lines) > 0, \"No claims extracted\"\n","\n","assert os.path.exists(EMB_NPY), \"Missing embeddings file\"\n","arr = np.load(EMB_NPY)\n","\n","assert arr.ndim == 2 and arr.shape[0] == len(lines), \"Embeddings size mismatch\"\n","\n","print(f\"‚úÖ Claims+Embeddings smoke test passed. {len(lines)} claims, embeddings shape = {arr.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n4_mO8_AdUK9","executionInfo":{"status":"ok","timestamp":1757874050688,"user_tz":-60,"elapsed":47,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"outputId":"66ed41bf-7080-473b-d5f2-7f777eb3a5d1"},"id":"n4_mO8_AdUK9","execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Claims+Embeddings smoke test passed. 109 claims, embeddings shape = (109, 1536)\n"]}]},{"cell_type":"markdown","source":["## Snapshot (versions, row count, duration) + pointer JSON"],"metadata":{"id":"tyKdTBw0UCqj"},"id":"tyKdTBw0UCqj"},{"cell_type":"code","source":["# === FACTR_04 Claims+Embeddings Snapshot ===\n","import os, json, time, numpy as np\n","\n","ROOT = \"/content/drive/MyDrive/FATCR\"\n","DATA_DIR = f\"{ROOT}/data/processed\"\n","SNAP_DIR = f\"{ROOT}/snapshots\"\n","CLAIMS_JSON = f\"{DATA_DIR}/CLAIMS_raw.jsonl\"\n","EMB_NPY = f\"{DATA_DIR}/CLAIMS_embeddings.npy\"\n","META_JSON = f\"{DATA_DIR}/CLAIMS_embeddings.meta.json\"\n","PTR_PATH  = f\"{DATA_DIR}/LAST_CLAIMS.json\"\n","\n","# ---- Checks ----\n","assert os.path.exists(CLAIMS_JSON), f\"Missing {CLAIMS_JSON}\"\n","assert os.path.exists(EMB_NPY), f\"Missing {EMB_NPY}\"\n","\n","lines = [x for x in open(CLAIMS_JSON, \"r\", encoding=\"utf-8\").read().splitlines() if x.strip()]\n","arr = np.load(EMB_NPY)\n","\n","assert len(lines) > 0, \"No claims extracted\"\n","assert arr.ndim == 2 and arr.shape[0] == len(lines), \"Embeddings size mismatch\"\n","\n","print(\"‚úÖ Claims+Embeddings snapshot\")\n","print(\"   Claims     :\", len(lines))\n","print(\"   Embeddings :\", arr.shape)\n","\n","# ---- Save snapshot ----\n","snap = {\n","    \"ts\"    : time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n","    \"claims\": len(lines),\n","    \"embeddings_shape\": arr.shape,\n","    \"raw_json\": os.path.relpath(CLAIMS_JSON, ROOT),\n","    \"embeddings_npy\": os.path.relpath(EMB_NPY, ROOT),\n","    \"meta_json\": os.path.relpath(META_JSON, ROOT),\n","}\n","\n","os.makedirs(SNAP_DIR, exist_ok=True)\n","snap_path = f\"{SNAP_DIR}/CLAIMS_SNAPSHOT_{int(time.time())}.json\"\n","with open(snap_path, \"w\") as f:\n","    json.dump(snap, f, indent=2)\n","print(\"üìù Saved snapshot ->\", os.path.relpath(snap_path, ROOT))\n","\n","# also write a small pointer JSON for git commits\n","with open(PTR_PATH, \"w\") as f:\n","    json.dump({\n","        \"ts\"    : snap[\"ts\"],\n","        \"claims\": len(lines),\n","        \"shape\" : arr.shape,\n","        \"path\"  : os.path.relpath(CLAIMS_JSON, ROOT),\n","    }, f, indent=2)\n","print(\"üîó Wrote pointer JSON ->\", os.path.relpath(PTR_PATH, ROOT))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"srhfuRb1eduv","executionInfo":{"status":"ok","timestamp":1757874350363,"user_tz":-60,"elapsed":37,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"outputId":"86ae8505-bfe6-45a7-f858-5e1aa22b3647"},"id":"srhfuRb1eduv","execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Claims+Embeddings snapshot\n","   Claims     : 109\n","   Embeddings : (109, 1536)\n","üìù Saved snapshot -> snapshots/CLAIMS_SNAPSHOT_1757874346.json\n","üîó Wrote pointer JSON -> data/processed/LAST_CLAIMS.json\n"]}]},{"cell_type":"markdown","source":["## Git push helper (commit notebook + pointer JSON + snapshots)"],"metadata":{"id":"O8FmQb_1UIof"},"id":"O8FmQb_1UIof"},{"cell_type":"code","source":["# # === FACTR push (commit notebook + pointer JSON + snapshots) ===\n","# from google.colab import userdata\n","# import urllib.parse, os, subprocess, shlex, time\n","\n","# ROOT = \"/content/drive/MyDrive/FATCR\"\n","# os.chdir(ROOT)\n","\n","# # Ensure git identity (set once per runtime)\n","# !git config --global user.email \"lukmaan@example.com\"\n","# !git config --global user.name \"Lukmaan Viscomi\"\n","\n","# print(\"üìÇ Repo status:\")\n","# !git status -sb\n","\n","# print(\"\\nüîÑ Pulling (rebase)‚Ä¶\")\n","# pat = userdata.get(\"GITHUB_PAT\")\n","# assert pat, \"Missing GITHUB_PAT in Colab Secrets.\"\n","# enc_pat = urllib.parse.quote(pat, safe=\"\")\n","# PULL_URL = f\"https://LukmaanViscomi:{enc_pat}@github.com/LukmaanViscomi/FATCR.git\"\n","# !git pull --rebase --autostash {PULL_URL} main || true\n","\n","# print(\"\\n‚ûï Staging files‚Ä¶\")\n","# !git add notebooks snapshots data/processed/LAST_UTTERANCES.json data/processed/LAST_CLAIMS.json README.md .gitignore 2>/dev/null || true\n","\n","# changed = subprocess.run([\"git\", \"diff\", \"--cached\", \"--quiet\"]).returncode != 0\n","# if changed:\n","#     msg = f\"FACTR_04: snapshots + pointers update [{int(time.time())}]\"\n","#     print(\"\\n‚úèÔ∏è Commit:\", msg)\n","#     !git commit -m {shlex.quote(msg)}\n","# else:\n","#     print(\"\\n‚ÑπÔ∏è Nothing new to commit.\")\n","\n","# print(\"\\n‚¨ÜÔ∏è Pushing to main‚Ä¶\")\n","# !git push {PULL_URL} HEAD:main\n","\n","# print(\"\\n‚úÖ Push complete.\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KzqQrWeIfns9","executionInfo":{"status":"ok","timestamp":1757874772505,"user_tz":-60,"elapsed":7573,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"outputId":"0d1d3bc0-f16c-44f1-fd3f-dd7a353ba5a7"},"id":"KzqQrWeIfns9","execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["üìÇ Repo status:\n","## \u001b[32mmain\u001b[m...\u001b[31morigin/main\u001b[m [ahead \u001b[32m1\u001b[m]\n","\u001b[32mA\u001b[m  data/processed/LAST_ASR.json\n"," \u001b[31mD\u001b[m notebooks/FACTR_02_Ingest_v2025-09-07_1.0.ipynb\n","\u001b[32mM\u001b[m  notebooks/FACTR_03_ASR+Diarize_v2025-09-07_1.0.ipynb\n"," \u001b[31mM\u001b[m notebooks/FACTR_04_Claims+Embeddings_v2025-09-07_1.0.ipynb\n","\u001b[31m??\u001b[m data/processed/CLAIMS_embeddings.meta.json\n","\u001b[31m??\u001b[m data/processed/LAST_CLAIMS.json\n","\u001b[31m??\u001b[m data/processed/LAST_INGEST.json\n","\u001b[31m??\u001b[m data/processed/LAST_INGEST.json,old\n","\u001b[31m??\u001b[m notebooks/FACTR_02_Ingest_v2025-09-07_2.0.ipynb\n","\n","üîÑ Pulling (rebase)‚Ä¶\n","From https://github.com/LukmaanViscomi/FATCR\n"," * branch            main       -> FETCH_HEAD\n","Already up to date.\n","\n","‚ûï Staging files‚Ä¶\n","\n","‚úèÔ∏è Commit: FACTR_04: snapshots + pointers update [1757874763]\n","[main 310cdab] FACTR_04: snapshots + pointers update [1757874763]\n"," 2 files changed, 7 insertions(+), 1 deletion(-)\n"," create mode 100644 data/processed/LAST_ASR.json\n"," rewrite notebooks/FACTR_03_ASR+Diarize_v2025-09-07_1.0.ipynb (98%)\n","\n","‚¨ÜÔ∏è Pushing to main‚Ä¶\n","Enumerating objects: 10, done.\n","Counting objects: 100% (10/10), done.\n","Delta compression using up to 2 threads\n","Compressing objects: 100% (5/5), done.\n","Writing objects: 100% (7/7), 1.80 KiB | 34.00 KiB/s, done.\n","Total 7 (delta 2), reused 0 (delta 0), pack-reused 0\n","remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n","To https://github.com/LukmaanViscomi/FATCR.git\n","   ffdd3fb..310cdab  HEAD -> main\n","\n","‚úÖ Push complete.\n"]}]},{"cell_type":"code","source":["# !git tag -a v2025-09-14 -m \"FACTR_03 ‚Üí FACTR_04 pipeline working end-to-end\"\n"],"metadata":{"id":"GrR3iadlh0o3","executionInfo":{"status":"ok","timestamp":1757875231373,"user_tz":-60,"elapsed":102,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}}},"id":"GrR3iadlh0o3","execution_count":28,"outputs":[]},{"cell_type":"code","source":["# !git push origin v2025-09-14\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SG1lMGifh2PS","executionInfo":{"status":"ok","timestamp":1757875241045,"user_tz":-60,"elapsed":1025,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"outputId":"379c48d7-dacb-44cf-99a6-fec685b00664"},"id":"SG1lMGifh2PS","execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Enumerating objects: 1, done.\n","Counting objects: 100% (1/1)\rCounting objects: 100% (1/1), done.\n","Writing objects: 100% (1/1)\rWriting objects: 100% (1/1), 204 bytes | 20.00 KiB/s, done.\n","Total 1 (delta 0), reused 0 (delta 0), pack-reused 0\n","To https://github.com/LukmaanViscomi/FATCR.git\n"," * [new tag]         v2025-09-14 -> v2025-09-14\n"]}]},{"cell_type":"code","source":["# !git tag\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sdduPTbUh5rN","executionInfo":{"status":"ok","timestamp":1757875250612,"user_tz":-60,"elapsed":105,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"outputId":"26c5d33b-bf26-4332-a269-29b8cc2bd14a"},"id":"sdduPTbUh5rN","execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["v2025-09-14\n"]}]},{"cell_type":"code","source":["# === FACTR push (commit notebook + pointer JSON + snapshots + optional tag) ===\n","from google.colab import userdata\n","import urllib.parse, os, subprocess, shlex\n","\n","ROOT = \"/content/drive/MyDrive/FATCR\"\n","os.chdir(ROOT)\n","\n","# --- Config ---\n","MILESTONE = \"\"   # e.g., \"v1.0-2025-09-14\" (leave blank if no tag wanted)\n","\n","# Show current status first\n","print(\"üìÇ Repo status:\")\n","!git status -sb\n","\n","# Pull (rebase) to avoid non-fast-forward errors\n","print(\"\\nüîÑ Pulling (rebase)‚Ä¶\")\n","pat = userdata.get(\"GITHUB_PAT\")\n","assert pat, \"Missing GITHUB_PAT in Colab Secrets.\"\n","enc_pat = urllib.parse.quote(pat, safe=\"\")\n","PULL_URL = f\"https://LukmaanViscomi:{enc_pat}@github.com/LukmaanViscomi/FATCR.git\"\n","!git pull --rebase {PULL_URL} main || true\n","\n","# Stage tracked files\n","print(\"\\n‚ûï Staging files‚Ä¶\")\n","!git add notebooks snapshots data/processed/LAST_*.json README.md .gitignore 2>/dev/null || true\n","\n","# Commit only if there are changes\n","changed = subprocess.run([\"git\", \"diff\", \"--cached\", \"--quiet\"]).returncode != 0\n","if changed:\n","    msg = \"FACTR: snapshot + pointer update\"\n","    print(\"\\n‚úèÔ∏è Commit:\", msg)\n","    !git commit -m {shlex.quote(msg)}\n","else:\n","    print(\"\\n‚ÑπÔ∏è Nothing new to commit.\")\n","\n","# Push (inject PAT only for the network call)\n","print(\"\\n‚¨ÜÔ∏è Pushing to main‚Ä¶\")\n","!git push {PULL_URL} HEAD:main\n","\n","# --- Optional: Milestone tag ---\n","if MILESTONE:\n","    print(f\"\\nüè∑Ô∏è Creating tag: {MILESTONE}\")\n","    # create/update the tag locally\n","    subprocess.run([\"git\", \"tag\", \"-f\", MILESTONE], check=True)\n","    # push the tag to GitHub\n","    subprocess.run([\"git\", \"push\", \"origin\", MILESTONE], check=True)\n","    print(\"‚úÖ Tag pushed:\", MILESTONE)\n","\n","print(\"\\n‚úÖ Push complete.\")\n"],"metadata":{"id":"q872dkfglHBK"},"id":"q872dkfglHBK","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","pygments_lexer":"ipython3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}