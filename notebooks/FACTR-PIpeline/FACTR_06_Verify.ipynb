{"cells":[{"cell_type":"markdown","id":"a7a62afe","metadata":{"id":"a7a62afe"},"source":["# FACTR_06 â€” Verify claims against Ground-Truth KB\n","\n","For each extracted **claim**, retrieve KB passages and judge **supports / contradicts / insufficient**, with evidence.\n","\n","**Inputs**: `CLAIMS_raw.jsonl` + `KB.faiss`/`KB.index.json`/`KB_passages.jsonl`\n","\n","**Output**: `VERIFICATION.jsonl`\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3R4tqqNAerwW","executionInfo":{"status":"ok","timestamp":1758377740607,"user_tz":-60,"elapsed":15624,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"outputId":"c12d2368-4f75-42df-80f0-d3e29b7a64fa"},"id":"3R4tqqNAerwW","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"id":"b3e8b805","metadata":{"id":"b3e8b805"},"outputs":[],"source":["# --- Setup & load artefacts\n","import os, json, faiss, numpy as np\n","\n","ROOT = \"/content/drive/MyDrive/FATCR\"\n","DATA_DIR = os.path.join(ROOT, \"data\", \"processed\")\n","\n","# Claims side\n","CLAIMS = os.path.join(DATA_DIR, \"CLAIMS_raw.jsonl\")\n","claims = [json.loads(l) for l in open(CLAIMS, \"r\", encoding=\"utf-8\") if l.strip()]\n","\n","# KB side\n","KB_FAISS = os.path.join(DATA_DIR, \"KB.faiss\")\n","KB_MAP   = os.path.join(DATA_DIR, \"KB.index.json\")\n","KB_PASS  = os.path.join(DATA_DIR, \"KB_passages.jsonl\")\n","\n","index_kb = faiss.read_index(KB_FAISS)\n","kb_map = json.load(open(KB_MAP, \"r\", encoding=\"utf-8\"))\n","kb_rows = [json.loads(l) for l in open(KB_PASS, \"r\", encoding=\"utf-8\") if l.strip()]\n","\n","print(\"Loaded:\", len(claims), \"claims;\", index_kb.ntotal, \"KB passages.\")\n"]},{"cell_type":"markdown","id":"9745fc8f","metadata":{"id":"9745fc8f"},"source":["## Embedder (copy of 05 logic)"]},{"cell_type":"code","execution_count":null,"id":"48f64849","metadata":{"id":"48f64849"},"outputs":[],"source":["import numpy as np, os, json\n","try:\n","    from google.colab import userdata  # type: ignore\n","    openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n","except Exception:\n","    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n","\n","EMBED_META = os.path.join(DATA_DIR, \"CLAIMS_embeddings.meta.json\")\n","embed_meta = json.load(open(EMBED_META, \"r\", encoding=\"utf-8\"))\n","model_name = embed_meta.get(\"model_name\") or embed_meta.get(\"model\", \"text-embedding-3-small\")\n","was_normalized = bool(embed_meta.get(\"normalized\", True))\n","metric = (embed_meta.get(\"faiss_metric\") or embed_meta.get(\"metric\") or \"ip\").lower()\n","\n","def _l2_normalize(vecs: np.ndarray) -> np.ndarray:\n","    norms = np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-8\n","    return vecs / norms\n","\n","def _is_openai_model(name: str) -> bool:\n","    return name.startswith(\"text-embedding-\")\n","\n","use_openai = _is_openai_model(model_name) and bool(openai_api_key)\n","print(\"Loading embedding model:\", model_name)\n","print(\"Using provider:\", \"OpenAI\" if use_openai else \"SentenceTransformers\")\n","\n","if use_openai:\n","    from openai import OpenAI\n","    client = OpenAI(api_key=openai_api_key)\n","    def embed_queries(texts):\n","        out = []\n","        for i in range(0, len(texts), 96):\n","            batch = texts[i:i+96]\n","            resp = client.embeddings.create(model=model_name, input=batch)\n","            out.extend([d.embedding for d in resp.data])\n","        vecs = np.asarray(out, dtype=\"float32\")\n","        return _l2_normalize(vecs) if was_normalized else vecs\n","else:\n","    if _is_openai_model(model_name) and not openai_api_key:\n","        print(\"Warning: OpenAI model requested but no key; falling back to MiniLM for testing only.\")\n","        model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n","    from sentence_transformers import SentenceTransformer\n","    st_model = SentenceTransformer(model_name)\n","    def embed_queries(texts):\n","        vecs = st_model.encode(texts, convert_to_numpy=True, normalize_embeddings=was_normalized)\n","        return vecs.astype(\"float32\")\n"]},{"cell_type":"markdown","id":"9d0f5ac7","metadata":{"id":"9d0f5ac7"},"source":["## Retrieval"]},{"cell_type":"code","execution_count":null,"id":"dfbf0d31","metadata":{"id":"dfbf0d31"},"outputs":[],"source":["def kb_search(text, k=20, prefilter=None):\n","    v = embed_queries([text])\n","    scores, ids = index_kb.search(v, k)\n","    out = []\n","    for s, fid in zip(scores[0], ids[0]):\n","        ridx = kb_map.get(str(int(fid)), int(fid))\n","        row = kb_rows[ridx]\n","        if (prefilter is None) or (float(s) >= prefilter):\n","            out.append({\"kb_id\": int(fid), \"score\": float(s), **row})\n","    return out\n"]},{"cell_type":"markdown","id":"d02853c2","metadata":{"id":"d02853c2"},"source":["## Option A: simple threshold verdict"]},{"cell_type":"code","execution_count":null,"id":"89743ab7","metadata":{"id":"89743ab7"},"outputs":[],"source":["def verify_claim_simple(claim_text, k=20, cosine_thresh=0.30):\n","    hits = kb_search(claim_text, k=k)\n","    supports = [h for h in hits if h[\"score\"] >= cosine_thresh]\n","    if supports:\n","        conf = min(1.0, supports[0][\"score\"] * 1.5 + 0.1 * (len(supports)-1))\n","        return {\"verdict\":\"supports\", \"confidence\": conf, \"evidence\": supports[:3]}\n","    return {\"verdict\":\"insufficient\", \"confidence\": 0.2, \"evidence\": hits[:3]}\n"]},{"cell_type":"markdown","id":"f8e15290","metadata":{"id":"f8e15290"},"source":["## Option B: NLI judge (supports/contradicts/insufficient)"]},{"cell_type":"code","execution_count":null,"id":"a2e72f40","metadata":{"id":"a2e72f40"},"outputs":[],"source":["# If needed on Colab:\n","# !pip -q install transformers torch\n","\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","import torch, torch.nn.functional as F\n","\n","nli_model = \"roberta-large-mnli\"  # or \"microsoft/deberta-large-mnli\"\n","tok = AutoTokenizer.from_pretrained(nli_model)\n","nli = AutoModelForSequenceClassification.from_pretrained(nli_model).eval()\n","\n","def nli_label(premise, hypothesis):\n","    x = tok(premise, hypothesis, return_tensors=\"pt\", truncation=True, max_length=512)\n","    with torch.no_grad():\n","        logits = nli(**x).logits\n","        probs = F.softmax(logits, dim=-1)[0].tolist()\n","    # roberta order: [contradiction, neutral, entailment]\n","    return {\"entail\": probs[2], \"contradict\": probs[0], \"neutral\": probs[1]}\n","\n","def verify_claim_nli(claim_text, k=12, prefilter=0.25, judge_top=5,\n","                     entail_thr=0.65, contra_thr=0.65):\n","    hits = kb_search(claim_text, k=k, prefilter=prefilter)[:judge_top]\n","    judged = []\n","    for h in hits:\n","        prem = h.get(\"text\") or \"\"\n","        jl = nli_label(prem, claim_text)\n","        h.update(jl)\n","        judged.append(h)\n","\n","    if not judged:\n","        return {\"verdict\":\"insufficient\", \"confidence\":0.2, \"evidence\":[]}\n","\n","    best_ent = max(judged, key=lambda r: r[\"entail\"])\n","    best_con = max(judged, key=lambda r: r[\"contradict\"])\n","\n","    if best_ent[\"entail\"] >= entail_thr:\n","        ev = sorted(judged, key=lambda r: -r[\"entail\"])[:3]\n","        return {\"verdict\":\"supports\", \"confidence\": float(best_ent[\"entail\"]), \"evidence\": ev}\n","\n","    if best_con[\"contradict\"] >= contra_thr:\n","        ev = sorted(judged, key=lambda r: -r[\"contradict\"])[:3]\n","        return {\"verdict\":\"contradicts\", \"confidence\": float(best_con[\"contradict\"]), \"evidence\": ev}\n","\n","    return {\"verdict\":\"insufficient\", \"confidence\": 0.4, \"evidence\": judged[:3]}\n"]},{"cell_type":"markdown","id":"17308586","metadata":{"id":"17308586"},"source":["## Run across all claims + save"]},{"cell_type":"code","execution_count":null,"id":"27b241ea","metadata":{"id":"27b241ea"},"outputs":[],"source":["OUT = os.path.join(DATA_DIR, \"VERIFICATION.jsonl\")\n","use_nli = True  # set False to use the simple threshold\n","\n","out_rows = []\n","for r in claims:\n","    txt = r.get(\"claim_text\") or r.get(\"claim\") or r.get(\"text\") or \"\"\n","    v = verify_claim_nli(txt, k=12) if use_nli else verify_claim_simple(txt, k=20)\n","    out_rows.append({\n","        \"claim_id\": r.get(\"row_id\") or r.get(\"id\"),\n","        \"claim_text\": txt,\n","        **v\n","    })\n","\n","with open(OUT, \"w\", encoding=\"utf-8\") as f:\n","    for row in out_rows:\n","        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n","\n","print(\"Wrote:\", OUT, \"| rows:\", len(out_rows))\n"]},{"cell_type":"markdown","id":"af94257a","metadata":{"id":"af94257a"},"source":["## Quick browse + summary"]},{"cell_type":"code","execution_count":null,"id":"2710f4fd","metadata":{"id":"2710f4fd"},"outputs":[],"source":["import pandas as pd\n","dfv = pd.read_json(OUT, lines=True)\n","display(dfv.head(10))\n","print(\"\\nVerdict distribution:\")\n","print(dfv[\"verdict\"].value_counts(normalize=True).round(3))\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}