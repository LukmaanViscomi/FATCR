{"cells":[{"cell_type":"markdown","id":"fdfb8d7a","metadata":{"id":"fdfb8d7a"},"source":["# FACTR_02 — KB Ingest (Quran/Hadith/Tafsir/etc.)\n","Build a ground-truth vector store using the **same embedding model** and **normalisation** as your claims index.\n","\n","**Outputs** under `data/processed/`:\n","- `KB_passages.jsonl`\n","- `KB_embeddings.npy`\n","- `KB_embeddings.meta.json`\n","- `KB.faiss`\n","- `KB.index.json`\n","- `LAST_KB.json`\n"]},{"cell_type":"code","execution_count":null,"id":"80e40af3","metadata":{"id":"80e40af3"},"outputs":[],"source":["# --- Setup\n","import os, json, re, numpy as np\n","\n","ROOT = \"/content/drive/MyDrive/FATCR\"\n","DATA_DIR = os.path.join(ROOT, \"data\", \"processed\")\n","RAW_KB_DIR = os.path.join(ROOT, \"data\", \"raw\", \"kb\")  # put sources here\n","os.makedirs(DATA_DIR, exist_ok=True)\n","print(\"RAW_KB_DIR:\", RAW_KB_DIR)\n","print(\"DATA_DIR:\", DATA_DIR)\n"]},{"cell_type":"markdown","id":"1c6ccfd7","metadata":{"id":"1c6ccfd7"},"source":["## Loaders & normalisers"]},{"cell_type":"code","execution_count":null,"id":"a8935e0d","metadata":{"id":"a8935e0d"},"outputs":[],"source":["import pandas as pd\n","\n","def _clean(s):\n","    import re\n","    return re.sub(r\"\\s+\", \" \", str(s)).strip()\n","\n","def load_jsonl(path, source):\n","    rows = []\n","    with open(path, \"r\", encoding=\"utf-8\") as f:\n","        for line in f:\n","            if not line.strip():\n","                continue\n","            r = json.loads(line)\n","            text = _clean(r.get(\"text\") or r.get(\"passage\") or \"\")\n","            if not text:\n","                continue\n","            rows.append({\n","                \"source\": source,\n","                \"collection\": _clean(r.get(\"collection\") or r.get(\"book\") or source),\n","                \"book\": _clean(r.get(\"book\") or r.get(\"surah\") or r.get(\"work\") or \"\"),\n","                \"chapter\": r.get(\"chapter\"),\n","                \"verse\": r.get(\"verse\"),\n","                \"number\": r.get(\"number\"),\n","                \"grade\": r.get(\"grade\"),\n","                \"lang\": r.get(\"lang\") or \"en\",\n","                \"ref\": _clean(r.get(\"ref\") or \"\"),\n","                \"text\": text\n","            })\n","    return rows\n","\n","def load_csv(path, source):\n","    df = pd.read_csv(path)\n","    rows = []\n","    for _, r in df.iterrows():\n","        text = _clean(r.get(\"text\") or r.get(\"passage\") or \"\")\n","        if not text:\n","            continue\n","        rows.append({\n","            \"source\": source,\n","            \"collection\": _clean(r.get(\"collection\") or r.get(\"book\") or source),\n","            \"book\": _clean(r.get(\"book\") or \"\"),\n","            \"chapter\": r.get(\"chapter\"),\n","            \"verse\": r.get(\"verse\"),\n","            \"number\": r.get(\"number\"),\n","            \"grade\": r.get(\"grade\"),\n","            \"lang\": r.get(\"lang\") or \"en\",\n","            \"ref\": _clean(r.get(\"ref\") or \"\"),\n","            \"text\": text\n","        })\n","    return rows\n","\n","def load_txt_paragraphs(path, source, book=\"\"):\n","    # Split on blank lines -> paragraphs (good for tafsir/exegesis)\n","    text = open(path, \"r\", encoding=\"utf-8\").read()\n","    import re\n","    paras = [_clean(p) for p in re.split(r\"\\n\\s*\\n\", text) if _clean(p)]\n","    out = []\n","    for p in paras:\n","        out.append({\n","            \"source\": source, \"collection\": source, \"book\": book,\n","            \"chapter\": None, \"verse\": None, \"number\": None, \"grade\": None,\n","            \"lang\": \"en\", \"ref\": \"\", \"text\": p\n","        })\n","    return out\n","\n","def harvest_kb(raw_dir):\n","    all_rows = []\n","    for root, _, files in os.walk(raw_dir):\n","        for name in files:\n","            p = os.path.join(root, name)\n","            lname = name.lower()\n","            src = os.path.basename(root)\n","            try:\n","                if lname.endswith(\".jsonl\"):\n","                    all_rows += load_jsonl(p, src)\n","                elif lname.endswith(\".csv\"):\n","                    all_rows += load_csv(p, src)\n","                elif lname.endswith(\".txt\"):\n","                    all_rows += load_txt_paragraphs(p, src, book=os.path.splitext(name)[0])\n","            except Exception as e:\n","                print(\"Skipping\", p, \"->\", e)\n","    return all_rows\n","\n","kb_rows = harvest_kb(RAW_KB_DIR)\n","print(\"Loaded KB rows:\", len(kb_rows))\n","kb_rows[:2]\n"]},{"cell_type":"markdown","id":"ff552581","metadata":{"id":"ff552581"},"source":["## Persist as `KB_passages.jsonl`"]},{"cell_type":"code","execution_count":null,"id":"acdf2cd1","metadata":{"id":"acdf2cd1"},"outputs":[],"source":["KB_PASS = os.path.join(DATA_DIR, \"KB_passages.jsonl\")\n","\n","with open(KB_PASS, \"w\", encoding=\"utf-8\") as f:\n","    for i, r in enumerate(kb_rows):\n","        r_out = {**r, \"id\": i}\n","        f.write(json.dumps(r_out, ensure_ascii=False) + \"\\n\")\n","\n","print(\"Wrote:\", KB_PASS, \"| count:\", len(kb_rows))\n"]},{"cell_type":"markdown","id":"d8ce8986","metadata":{"id":"d8ce8986"},"source":["## Embed (same model + normalisation as CLAIMS)"]},{"cell_type":"code","execution_count":null,"id":"4812e77a","metadata":{"id":"4812e77a"},"outputs":[],"source":["# Try Colab Secrets first, then env var\n","try:\n","    from google.colab import userdata  # type: ignore\n","    openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n","except Exception:\n","    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n","\n","model_name = \"text-embedding-3-small\"  # keep same as claims\n","was_normalized = True\n","metric = \"ip\"\n","\n","def _is_openai_model(name: str) -> bool:\n","    return name.startswith(\"text-embedding-\")\n","\n","def embed_openai(texts, model):\n","    from openai import OpenAI\n","    import numpy as np\n","    client = OpenAI(api_key=openai_api_key)\n","    out = []\n","    for i in range(0, len(texts), 96):\n","        batch = texts[i:i+96]\n","        resp = client.embeddings.create(model=model, input=batch)\n","        out.extend([d.embedding for d in resp.data])\n","    arr = np.asarray(out, dtype=\"float32\")\n","    norms = np.linalg.norm(arr, axis=1, keepdims=True) + 1e-8\n","    return arr / norms\n","\n","def embed_st(texts, model):\n","    from sentence_transformers import SentenceTransformer\n","    st = SentenceTransformer(model)\n","    arr = st.encode(texts, convert_to_numpy=True, normalize_embeddings=was_normalized)\n","    return arr.astype(\"float32\")\n","\n","texts = [json.loads(l)[\"text\"] for l in open(KB_PASS, \"r\", encoding=\"utf-8\") if l.strip()]\n","if _is_openai_model(model_name) and openai_api_key:\n","    print(\"Using provider: OpenAI\")\n","    vecs = embed_openai(texts, model_name)\n","else:\n","    print(\"Using provider: SentenceTransformers (fallback MiniLM)\")\n","    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n","    vecs = embed_st(texts, model_name)\n","\n","KB_EMB  = os.path.join(DATA_DIR, \"KB_embeddings.npy\")\n","np.save(KB_EMB, vecs)\n","KB_META = os.path.join(DATA_DIR, \"KB_embeddings.meta.json\")\n","meta = {\"model_name\": model_name, \"normalized\": was_normalized, \"faiss_metric\": metric,\n","        \"count\": len(texts), \"dim\": int(vecs.shape[1])}\n","json.dump(meta, open(KB_META, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n","print(\"Saved:\", KB_EMB, \"|\", KB_META, \"| shape:\", vecs.shape)\n"]},{"cell_type":"markdown","id":"45df41e4","metadata":{"id":"45df41e4"},"source":["## Build FAISS + id_map (identity) + pointer"]},{"cell_type":"code","execution_count":null,"id":"3140cf63","metadata":{"id":"3140cf63"},"outputs":[],"source":["import faiss, json, os, numpy as np\n","vecs = np.load(KB_EMB).astype(\"float32\")\n","if was_normalized:\n","    faiss.normalize_L2(vecs)\n","d = vecs.shape[1]\n","index = faiss.IndexFlatIP(d) if was_normalized else faiss.IndexFlatL2(d)\n","index.add(vecs)\n","print(\"index.ntotal:\", index.ntotal)\n","\n","KB_FAISS = os.path.join(DATA_DIR, \"KB.faiss\")\n","KB_MAP   = os.path.join(DATA_DIR, \"KB.index.json\")\n","faiss.write_index(index, KB_FAISS)\n","id_map = {str(i): i for i in range(index.ntotal)}\n","json.dump(id_map, open(KB_MAP, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n","assert index.ntotal == len(id_map) == meta[\"count\"], \"Inconsistent counts for KB index/id_map/meta\"\n","\n","LAST_KB = {\n","    \"artefacts\": {\n","        \"kb_faiss\": os.path.relpath(KB_FAISS, ROOT),\n","        \"kb_index_json\": os.path.relpath(KB_MAP, ROOT),\n","        \"kb_passages\": os.path.relpath(KB_PASS, ROOT),\n","    },\n","    \"dim\": d, \"normalized\": was_normalized, \"model\": model_name\n","}\n","json.dump(LAST_KB, open(os.path.join(DATA_DIR, \"LAST_KB.json\"), \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n","print(\"✅ KB index built and consistent.\")\n"]},{"cell_type":"markdown","id":"fffbeb04","metadata":{"id":"fffbeb04"},"source":["## Smoke test"]},{"cell_type":"code","execution_count":null,"id":"e9f43f44","metadata":{"id":"e9f43f44"},"outputs":[],"source":["def kb_search(text, k=5):\n","    if model_name.startswith(\"text-embedding-\") and (openai_api_key is not None):\n","        from numpy import array\n","        # embed_openai returns already-normalised vectors\n","        v = embed_openai([text], model_name)\n","    else:\n","        v = embed_st([text], model_name)\n","    scores, ids = index.search(v, k)\n","    rows = [json.loads(l) for l in open(KB_PASS, \"r\", encoding=\"utf-8\") if l.strip()]\n","    out = []\n","    for s, fid in zip(scores[0], ids[0]):\n","        out.append({\"kb_id\": int(fid), \"score\": float(s), **rows[id_map[str(int(fid))]]})\n","    import pandas as pd\n","    return pd.DataFrame(out)\n","\n","kb_search(\"two natures\", k=5).head()\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}