{"cells":[{"cell_type":"markdown","id":"eoRI5uPbYK2_","metadata":{"id":"eoRI5uPbYK2_"},"source":["# FACTR ‚Äî ASR (faster-whisper) + Diarize/Align (WhisperX)\n","**Version:** v2025-09-07_1.0  \n","**Purpose:** Turn AUDIO_PATH into UTTERANCES.parquet with speakers & timestamps.\n"]},{"cell_type":"markdown","id":"owl1cFyceYKl","metadata":{"id":"owl1cFyceYKl"},"source":["# Why these pins / choices?\n","\n","We avoid the old ‚ÄúTranscriptionOptions(..., multilingual=‚Ä¶)‚Äù mismatch by not calling WhisperX‚Äôs ASR; instead we use faster-whisper for ASR and keep WhisperX just for alignment & diarization, which works cleanly with:\n","\n","faster-whisper==1.1.1\n","\n","ctranslate2==4.4.0\n","\n","onnxruntime==1.19.2\n","\n","whisperx@git + pyannote.audio==3.3.2\n","\n","The ASR model list tries medium.en then falls back to small.en; on CPU it uses small.en.\n","\n","compute_type=\"int8_float16\" (GPU) or \"int8\" (CPU) keeps VRAM/RAM in check and reduces OOMs.\n","\n","vad_filter=False avoids pulling an extra VAD model; diarization handles speaker turns anyway."]},{"cell_type":"code","execution_count":1,"id":"qDQfEoDwk8GY","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1063,"status":"ok","timestamp":1757830867165,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"},"user_tz":-60},"id":"qDQfEoDwk8GY","outputId":"8c70c061-3004-46e9-b98a-341ff1f6f811"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","id":"v8Dsawwidvij","metadata":{"id":"v8Dsawwidvij"},"source":["## üîß Section 0 ‚Äî One-cell setup (installs & sanity print)"]},{"cell_type":"code","execution_count":2,"id":"yTNX6u1edzHU","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yTNX6u1edzHU","executionInfo":{"status":"ok","timestamp":1757831291026,"user_tz":-60,"elapsed":423863,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"outputId":"4a706ba6-1a66-4ea0-9896-4fdbe0587afd"},"outputs":[{"output_type":"stream","name":"stdout","text":["pyannote-metrics 4.0.0 has requirement numpy>=2.2.2, but you have numpy 2.0.2.\n","torchvision 0.20.1 has requirement torch==2.5.1, but you have torch 2.8.0.\n","google-colab 1.0.0 has requirement pandas==2.2.2, but you have pandas 2.3.2.\n","google-colab 1.0.0 has requirement requests==2.32.4, but you have requests 2.32.5.\n","tensorflow 2.19.0 has requirement protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.32.1.\n","google-ai-generativelanguage 0.6.15 has requirement protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.32.1.\n","grpcio-status 1.71.2 has requirement protobuf<6.0dev,>=5.26.1, but you have protobuf 6.32.1.\n","cudf-cu12 25.6.0 has requirement pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2.\n","bigframes 2.18.0 has requirement rich<14,>=12.4.4, but you have rich 14.1.0.\n","dask-cudf-cu12 25.6.0 has requirement pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2.\n","datasets 4.0.0 has requirement fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.9.0.\n","gcsfs 2025.3.0 has requirement fsspec==2025.3.0, but you have fsspec 2025.9.0.\n"]},{"output_type":"stream","name":"stderr","text":["ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","pyannote-metrics 4.0.0 requires numpy>=2.2.2, but you have numpy 2.0.2 which is incompatible.\n","torchvision 0.20.1 requires torch==2.5.1, but you have torch 2.8.0 which is incompatible.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n","google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n","tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.32.1 which is incompatible.\n","bigframes 2.18.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\n","datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.9.0 which is incompatible.\n","ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n","google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n","opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\n","tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.3 which is incompatible.\n","tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.32.1 which is incompatible.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\n","numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.3 which is incompatible.\n","cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.3 which is incompatible.\n","opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\n","bigframes 2.18.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\n","datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.9.0 which is incompatible.\n","gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\n","ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torch 2.5.1 requires sympy==1.13.1; python_version >= \"3.9\", but you have sympy 1.14.0 which is incompatible.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n","google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n","opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\n","tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.3 which is incompatible.\n","tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.32.1 which is incompatible.\n","google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.32.1 which is incompatible.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\n","numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.3 which is incompatible.\n","grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.32.1 which is incompatible.\n","cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.3 which is incompatible.\n","opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\n","bigframes 2.18.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\n","datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.9.0 which is incompatible.\n","gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\n","  DEPRECATION: Building 'antlr4-python3-runtime' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'antlr4-python3-runtime'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n","  DEPRECATION: Building 'julius' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'julius'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n","ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.20.1 requires torch==2.5.1, but you have torch 2.8.0 which is incompatible.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.2 which is incompatible.\n","google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n","opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\n","tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.3 which is incompatible.\n","tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.32.1 which is incompatible.\n","google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.32.1 which is incompatible.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\n","numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.3 which is incompatible.\n","grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.32.1 which is incompatible.\n","cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.3 which is incompatible.\n","cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2 which is incompatible.\n","opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\n","bigframes 2.18.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\n","dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2 which is incompatible.\n","datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.9.0 which is incompatible.\n","gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\n","ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","pyannote-metrics 4.0.0 requires numpy>=2.2.2, but you have numpy 2.0.2 which is incompatible.\n","torchvision 0.20.1 requires torch==2.5.1, but you have torch 2.8.0 which is incompatible.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.2 which is incompatible.\n","google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n","tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.32.1 which is incompatible.\n","cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2 which is incompatible.\n","bigframes 2.18.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\n","dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2 which is incompatible.\n","datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.9.0 which is incompatible.\n","ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","pyannote-metrics 4.0.0 requires numpy>=2.2.2, but you have numpy 2.0.2 which is incompatible.\n","torchvision 0.20.1 requires torch==2.5.1, but you have torch 2.8.0 which is incompatible.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.2 which is incompatible.\n","google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n","tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.32.1 which is incompatible.\n","cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2 which is incompatible.\n","bigframes 2.18.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\n","dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2 which is incompatible.\n","datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.9.0 which is incompatible.\n"]}],"source":["%%bash\n","set -euo pipefail\n","\n","# 0) Keep pip modern (but below the 25.3 change)\n","pip install -q --upgrade \"pip<25.3\" wheel\n","\n","# 1) Baseline scientific stack (match what faster-whisper/onnxruntime expect)\n","pip install -q --upgrade --force-reinstall --no-cache-dir \\\n","  \"numpy==2.0.2\" \"pandas==2.2.3\" \"pyarrow>=15,<17\" \"jedi>=0.16\"\n","\n","# 2) PyTorch trio (Colab will auto-pick a CUDA build if a GPU is attached)\n","pip install -q --upgrade --force-reinstall --no-cache-dir \\\n","  \"torch==2.5.1\" \"torchvision==0.20.1\" \"torchaudio==2.5.1\"\n","\n","# 3) ASR stack (faster-whisper + onnxruntime, versions that play nicely together)\n","pip install -q --upgrade --force-reinstall --no-cache-dir \\\n","  \"faster-whisper==1.1.1\" \"ctranslate2==4.4.0\" \"onnxruntime==1.19.2\"\n","\n","# 4) WhisperX + pyannote\n","pip install -q --upgrade --force-reinstall --no-cache-dir \\\n","  \"git+https://github.com/m-bain/whisperx.git\" \"pyannote.audio==3.3.2\"\n","\n","# 5) Utilities\n","pip install -q --upgrade librosa soundfile matplotlib\n","\n","# ‚úÖ 6) Re-pin NumPy last to avoid accidental upgrades during deps resolution\n","pip install -q --upgrade --force-reinstall --no-cache-dir \"numpy==2.0.2\"\n","\n","# Show real breakages (warnings here are ok)\n","pip check || true\n"]},{"cell_type":"code","execution_count":3,"id":"FO4H05vGd2ge","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FO4H05vGd2ge","executionInfo":{"status":"ok","timestamp":1757831305131,"user_tz":-60,"elapsed":14103,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"outputId":"fc7f2ad3-023b-4167-fb21-04ae011bb659"},"outputs":[{"output_type":"stream","name":"stdout","text":["Python: 3.12.11\n","CUDA available: True\n","Torch: 2.8.0+cu128\n","NumPy: 2.0.2\n","Pandas: 2.3.2\n","faster_whisper   1.2.0\n","ctranslate2      4.4.0\n","onnxruntime      1.22.1\n","whisperx         git\n","pyannote.audio   not importable -> operator torchvision::nms does not exist\n"]}],"source":["# --- Print versions so we can diff future runs quickly ---\n","import sys, importlib, torch, numpy as np, pandas as pd\n","mods = [\"faster_whisper\",\"ctranslate2\",\"onnxruntime\",\"whisperx\",\"pyannote.audio\"]\n","print(\"Python:\", sys.version.split()[0])\n","print(\"CUDA available:\", torch.cuda.is_available())\n","print(\"Torch:\", torch.__version__)\n","print(\"NumPy:\", np.__version__)\n","print(\"Pandas:\", pd.__version__)\n","for m in mods:\n","    try:\n","        mod = importlib.import_module(m if m!=\"pyannote.audio\" else \"pyannote.audio\")\n","        print(f\"{m:16s}\", getattr(mod, \"__version__\", \"git\"))\n","    except Exception as e:\n","        print(f\"{m:16s}\", \"not importable ->\", e)\n"]},{"cell_type":"markdown","id":"idWpT4Myd5Le","metadata":{"id":"idWpT4Myd5Le"},"source":["## üì• Section 1 ‚Äî Load the audio path from 02 (handoff)"]},{"cell_type":"code","execution_count":4,"id":"Om0fqJU3d67y","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Om0fqJU3d67y","executionInfo":{"status":"ok","timestamp":1757831305174,"user_tz":-60,"elapsed":38,"user":{"displayName":"Luca Viscomi","userId":"11057007974013554352"}},"outputId":"cecb83b2-1c62-491e-98b0-b45aaff36183"},"outputs":[{"output_type":"stream","name":"stdout","text":["AUDIO_PATH: /content/drive/MyDrive/FATCR/data/processed/speFWRuuJNs_16k_mono.wav\n"]}],"source":["import json, os\n","\n","# Expect this file from FACTR_02\n","HANDOFF = \"/content/drive/MyDrive/FATCR/data/processed/LAST_INGEST.json\"\n","assert os.path.exists(HANDOFF), \"LAST_INGEST.json not found (run FACTR_02 first).\"\n","\n","with open(HANDOFF, \"r\") as f:\n","    meta = json.load(f)\n","\n","# --- Fix: make AUDIO_PATH absolute ---\n","repo_root = \"/content/drive/MyDrive/FATCR\"\n","AUDIO_PATH = os.path.join(repo_root, meta[\"audio_path\"])\n","print(\"AUDIO_PATH:\", AUDIO_PATH)\n","\n","assert os.path.exists(AUDIO_PATH) and os.path.getsize(AUDIO_PATH) > 10_000, \"Bad AUDIO_PATH.\"\n","\n"]},{"cell_type":"markdown","id":"1i2vEjcZd_O6","metadata":{"id":"1i2vEjcZd_O6"},"source":["## üéôÔ∏è Section 2 ‚Äî ASR with faster-whisper (OOM-safe)"]},{"cell_type":"code","execution_count":null,"id":"hTof86tyeAYb","metadata":{"id":"hTof86tyeAYb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"72736ee5-13c6-4d3b-aa9f-ef3ceb3f9b30"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚Üí loading medium.en in faster-whisper on cuda (int8_float16)\n"]}],"source":["from faster_whisper import WhisperModel\n","import torch, gc\n","\n","HAS_CUDA = torch.cuda.is_available()\n","DEVICE = \"cuda\" if HAS_CUDA else \"cpu\"\n","compute_type = \"int8_float16\" if HAS_CUDA else \"int8\"  # conservative & OOM-friendly\n","\n","arch_candidates = [\"medium.en\", \"small.en\"] if HAS_CUDA else [\"small.en\"]\n","\n","fw_model = None\n","last_err = None\n","\n","for name in arch_candidates:\n","    try:\n","        print(f\"‚Üí loading {name} in faster-whisper on {DEVICE} ({compute_type})\")\n","        fw_model = WhisperModel(name, device=DEVICE, compute_type=compute_type)\n","        break\n","    except Exception as e:\n","        print(\"‚ö†Ô∏è load failed:\", e)\n","        last_err = e\n","        gc.collect()\n","        torch.cuda.empty_cache() if HAS_CUDA else None\n","\n","if fw_model is None:\n","    raise RuntimeError(f\"Could not load a faster-whisper model. Last error: {last_err}\")\n","\n","# Do the transcription (disable VAD to avoid extra model load; diarization will handle speech turns)\n","segments_gen, info = fw_model.transcribe(\n","    AUDIO_PATH,\n","    language=\"en\",           # set language if known to skip detection\n","    beam_size=5,\n","    vad_filter=False,        # True enables Silero-VAD; keep False unless you want pre-filtering\n",")\n","\n","# Convert to a simple list of segments\n","asr_segments = []\n","for s in segments_gen:\n","    asr_segments.append({\n","        \"start\": float(s.start) if s.start is not None else None,\n","        \"end\":   float(s.end)   if s.end   is not None else None,\n","        \"text\": (s.text or \"\").strip()\n","    })\n","\n","print(f\"ASR segments: {len(asr_segments)} | detected language: {info.language or 'en'}\")\n"]},{"cell_type":"code","source":["%%bash\n","set -euo pipefail\n","\n","# Detect GPU\n","python - <<'PY'\n","import torch, json, sys\n","print(json.dumps({\"has_cuda\": torch.cuda.is_available()}))\n","PY\n"],"metadata":{"id":"35eF0KABxnF8"},"id":"35eF0KABxnF8","execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%bash\n","set -euo pipefail\n","# Install the CPU builds explicitly\n","pip install -q --force-reinstall --no-cache-dir \\\n","  torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 \\\n","  --index-url https://download.pytorch.org/whl/cpu\n"],"metadata":{"id":"vn6sr1nlxw_J"},"id":"vn6sr1nlxw_J","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n","try:\n","    import torchaudio, torchvision\n","    print(\"Torchaudio:\", torchaudio.__version__)\n","    print(\"Torchvision:\", torchvision.__version__)\n","except Exception as e:\n","    print(\"Import error:\", repr(e))\n"],"metadata":{"id":"_UJz4bZyyQYz"},"id":"_UJz4bZyyQYz","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"88C9EtKfeE1a","metadata":{"id":"88C9EtKfeE1a"},"source":["## üìè Section 3 ‚Äî (Optional) alignment with WhisperX"]},{"cell_type":"code","execution_count":null,"id":"7DdCUHtoeGLA","metadata":{"id":"7DdCUHtoeGLA"},"outputs":[],"source":["import whisperx\n","\n","align_model, metadata = whisperx.load_align_model(\n","    language_code=(info.language or \"en\"),\n","    device=DEVICE\n",")\n","asr_aligned = whisperx.align(asr_segments, align_model, metadata, AUDIO_PATH, DEVICE)\n"]},{"cell_type":"code","source":["import whisperx\n","\n","# Pick the language: use 'info.language' if available, else default to English\n","language_code = \"en\"\n","try:\n","    if \"info\" in globals() and getattr(info, \"language\", None):\n","        language_code = info.language\n","except Exception:\n","    pass\n","\n","align_model, metadata = whisperx.load_align_model(\n","    language_code=language_code,\n","    device=DEVICE\n",")\n","\n","asr_aligned = whisperx.align(asr_segments, align_model, metadata, AUDIO_PATH, DEVICE)\n","print(\"‚úÖ Alignment complete with language:\", language_code)\n"],"metadata":{"id":"hQOml4DbzMHB"},"id":"hQOml4DbzMHB","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"cXj7A9KleJy0","metadata":{"id":"cXj7A9KleJy0"},"source":["## üó£Ô∏è Section 4 ‚Äî Diarization with WhisperX"]},{"cell_type":"code","execution_count":null,"id":"IWJXSHLzeK2T","metadata":{"id":"IWJXSHLzeK2T"},"outputs":[],"source":["# If you have a HF token for diarization models, add it here (optional)\n","HUGGINGFACE_TOKEN = \"\"  # e.g. \"hf_xxx\"; leave empty to use public pipeline\n","\n","if HUGGINGFACE_TOKEN:\n","    diar = whisperx.DiarizationPipeline(device=DEVICE, use_auth_token=HUGGINGFACE_TOKEN)\n","else:\n","    diar = whisperx.DiarizationPipeline(device=DEVICE)\n","\n","diar_out = diar(AUDIO_PATH)\n","\n","# Assign speakers to aligned words/segments\n","asr_spk = whisperx.assign_word_speakers(diar_out, asr_aligned)\n","print(\"Diarization done.\")\n"]},{"cell_type":"markdown","id":"fknVGvQrePE_","metadata":{"id":"fknVGvQrePE_"},"source":["## üíæ Section 5 ‚Äî Save as UTTERANCES.parquet"]},{"cell_type":"code","execution_count":null,"id":"5yPMeqWheQaa","metadata":{"id":"5yPMeqWheQaa"},"outputs":[],"source":["import pandas as pd, os\n","\n","rows = []\n","for seg in asr_spk[\"segments\"]:\n","    rows.append({\n","        \"video_id\": os.path.basename(AUDIO_PATH),\n","        \"t_start\":  seg.get(\"start\"),\n","        \"t_end\":    seg.get(\"end\"),\n","        \"speaker\":  seg.get(\"speaker\", \"SPEAKER_00\"),\n","        \"text\":     (seg.get(\"text\") or \"\").strip()\n","    })\n","\n","df_utts = pd.DataFrame(rows)\n","os.makedirs(\"data/processed\", exist_ok=True)\n","OUT_PARQUET = \"data/processed/UTTERANCES.parquet\"\n","df_utts.to_parquet(OUT_PARQUET, index=False)\n","print(f\"‚úÖ wrote {OUT_PARQUET} with\", len(df_utts), \"rows\")\n","df_utts.head()\n"]},{"cell_type":"markdown","id":"QBauMo5Pe19g","metadata":{"id":"QBauMo5Pe19g"},"source":["## 6) Snapshot + hand-off pointer (JSON)"]},{"cell_type":"code","execution_count":null,"id":"bo06u_Xye4oD","metadata":{"id":"bo06u_Xye4oD"},"outputs":[],"source":["# === 6) Snapshot + hand-off pointer ===\n","import json, time, platform, os, sys\n","import torch, pandas as pd\n","\n","# Basic metrics\n","dur_sec = float(df_utts[\"t_end\"].fillna(0).max() or 0)\n","row_count = int(len(df_utts))\n","\n","# Try to read versions safely\n","def safe_ver(modname):\n","    try:\n","        m = __import__(modname)\n","        return getattr(m, \"__version__\", \"git\")\n","    except Exception:\n","        return \"n/a\"\n","\n","snap = {\n","    \"ts\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n","    \"platform\": platform.platform(),\n","    \"python\": sys.version.split()[0],\n","    \"cuda_available\": torch.cuda.is_available(),\n","    \"device\": DEVICE,\n","    \"compute_type\": compute_type,\n","    \"asr_model\": getattr(fw_model, \"_model_name\", \"unknown\"),  # faster-whisper keeps name here\n","    \"whisperx\": safe_ver(\"whisperx\"),\n","    \"faster_whisper\": safe_ver(\"faster_whisper\"),\n","    \"ctranslate2\": safe_ver(\"ctranslate2\"),\n","    \"onnxruntime\": safe_ver(\"onnxruntime\"),\n","    \"pyannote_audio\": safe_ver(\"pyannote.audio\"),\n","    \"audio_path\": AUDIO_PATH,\n","    \"utterances_parquet\": OUT_PARQUET,\n","    \"rows\": row_count,\n","    \"duration_sec\": round(dur_sec, 2),\n","}\n","\n","os.makedirs(\"snapshots\", exist_ok=True)\n","snap_path = f\"snapshots/ASR_SNAPSHOT_{int(time.time())}.json\"\n","with open(snap_path, \"w\") as f:\n","    json.dump(snap, f, indent=2)\n","print(\"üóÇÔ∏è Snapshot:\", snap_path)\n","\n","# Hand-off pointer for downstream notebooks\n","handoff = {\n","    \"when\": snap[\"ts\"],\n","    \"audio_path\": AUDIO_PATH,\n","    \"utterances_parquet\": OUT_PARQUET,\n","    \"rows\": row_count,\n","    \"note\": \"Use 'utterances_parquet' for FACTR_04_Claims+Embeddings\",\n","}\n","with open(\"data/processed/LAST_ASR.json\", \"w\") as f:\n","    json.dump(handoff, f, indent=2)\n","print(\"üìù Wrote data/processed/LAST_ASR.json\")\n"]},{"cell_type":"markdown","id":"m_A5QSe7e-40","metadata":{"id":"m_A5QSe7e-40"},"source":["## 7) Optional: git-push helper (uses GITHUB_PAT in Colab Secrets)\n","\n","This commits your notebook, the snapshot, and the hand-off JSON.\n","If the parquet is > 20 MB, it avoids pushing it and commits only the pointer to keep the repo lean."]},{"cell_type":"code","execution_count":null,"id":"_er8Phzxe_6e","metadata":{"id":"_er8Phzxe_6e"},"outputs":[],"source":["# === 7) Optional push to GitHub (needs GITHUB_PAT in Colab Secrets) ===\n","from google.colab import userdata\n","import urllib.parse, subprocess, os, shlex\n","\n","REPO_DIR = \"/content/drive/MyDrive/FATCR\"\n","os.chdir(REPO_DIR)\n","\n","def run(cmd):\n","    print(\"$\", cmd)\n","    return subprocess.run(shlex.split(cmd), check=False)\n","\n","print(\"üìÇ Repo status before push:\")\n","run(\"git status -sb\")\n","\n","pat = userdata.get(\"GITHUB_PAT\")\n","if not pat:\n","    print(\"‚ÑπÔ∏è No GITHUB_PAT in Colab Secrets ‚Äî skipping push.\")\n","else:\n","    enc_pat = urllib.parse.quote(pat, safe=\"\")\n","    REMOTE_URL = f\"https://LukmaanViscomi:{enc_pat}@github.com/LukmaanViscomi/FATCR.git\"\n","\n","    # Always pull latest (rebase) to reduce non-fast-forward issues\n","    print(\"\\nüîÑ Pulling latest (rebase, autostash)‚Ä¶\")\n","    run(f\"git pull --rebase --autostash {REMOTE_URL} main\")\n","\n","    # Decide whether to add the parquet (skip if very large)\n","    parquet_size = os.path.getsize(OUT_PARQUET) if os.path.exists(OUT_PARQUET) else 0\n","    add_parquet = parquet_size <= 20 * 1024 * 1024  # 20 MB\n","\n","    # Stage files\n","    paths = [\n","        \"notebooks\",                 # your notebooks\n","        \"snapshots\",                 # ASR snapshot\n","        \"data/processed/LAST_ASR.json\",\n","    ]\n","    if add_parquet:\n","        paths.append(OUT_PARQUET)    # add parquet if small enough\n","    else:\n","        print(f\"‚ÑπÔ∏è {OUT_PARQUET} is {parquet_size/1e6:.1f} MB; skipping to keep repo small.\")\n","\n","    run(\"git add \" + \" \".join(shlex.quote(p) for p in paths) + \" .gitignore\")\n","\n","    # Commit only if there are staged changes\n","    changed = subprocess.run([\"git\",\"diff\",\"--cached\",\"--quiet\"]).returncode != 0\n","    if changed:\n","        msg = \"ASR+diarize results (snapshot + pointer)\"\n","        print(\"\\n‚úèÔ∏è Committing:\", msg)\n","        run(f'git commit -m \"{msg}\"')\n","        print(\"\\n‚¨ÜÔ∏è Pushing to main‚Ä¶\")\n","        run(f\"git push {REMOTE_URL} HEAD:main\")\n","        print(\"\\n‚úÖ Push complete.\")\n","    else:\n","        print(\"\\n‚ÑπÔ∏è Nothing to commit.\")\n"]},{"cell_type":"code","execution_count":null,"id":"pHUvWqBtYK3D","metadata":{"id":"pHUvWqBtYK3D"},"outputs":[],"source":["# Config (tune these safely)\n","class CFG:\n","    DEVICE = \"cuda\"   # \"cuda\" or \"cpu\"\n","    ASR_MODEL = \"small.en\"   # try \"medium.en\" later\n","    CHUNK_LENGTH = 20        # seconds\n","    BEAM_SIZE = 3\n","    WORD_TIMESTAMPS = True\n","    USE_ALIGNMENT = True\n","    USE_DIARIZATION = True\n","    HUGGINGFACE_TOKEN = \"\"   # optional\n","print(vars(CFG))\n"]},{"cell_type":"code","execution_count":null,"id":"-PhUQO4dYK3E","metadata":{"id":"-PhUQO4dYK3E"},"outputs":[],"source":["# ASR -> Diarize -> Align -> Save parquet\n","import os, gc, torch, pandas as pd\n","from faster_whisper import WhisperModel\n","import whisperx\n","\n","assert \"AUDIO_PATH\" in globals() and AUDIO_PATH and os.path.exists(AUDIO_PATH), \"AUDIO_PATH missing (run Ingest).\"\n","\n","# Cap native threads to avoid RAM spikes\n","os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n","os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n","os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n","os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n","\n","HAS_CUDA = torch.cuda.is_available() and (CFG.DEVICE == \"cuda\")\n","DEVICE = \"cuda\" if HAS_CUDA else \"cpu\"\n","compute_type = \"int8_float16\" if HAS_CUDA else \"int8\"\n","\n","print(f\"‚Üí loading {CFG.ASR_MODEL} in faster-whisper on {DEVICE} ({compute_type})\")\n","fw = WhisperModel(CFG.ASR_MODEL, device=DEVICE, compute_type=compute_type)\n","\n","segments_gen, info = fw.transcribe(\n","    AUDIO_PATH,\n","    language=\"en\",\n","    beam_size=CFG.BEAM_SIZE,\n","    vad_filter=False,   # let WhisperX diarization handle VAD\n","    chunk_length=CFG.CHUNK_LENGTH,\n","    word_timestamps=CFG.WORD_TIMESTAMPS,\n",")\n","\n","asr_segments = []\n","for s in segments_gen:\n","    seg = {\"start\": float(s.start) if s.start is not None else None,\n","           \"end\": float(s.end) if s.end is not None else None,\n","           \"text\": (s.text or \"\").strip()}\n","    if getattr(s, \"words\", None):\n","        seg[\"words\"] = [{\"start\": float(w.start) if w.start is not None else None,\n","                         \"end\": float(w.end) if w.end is not None else None,\n","                         \"word\": w.word} for w in s.words]\n","    asr_segments.append(seg)\n","asr = {\"segments\": asr_segments, \"language\": (info.language or \"en\")}\n","print(f\"ASR segments: {len(asr_segments)} | language: {asr['language']}\")\n","\n","# Diarization\n","if CFG.USE_DIARIZATION:\n","    try:\n","        from whisperx.diarize import DiarizationPipeline\n","    except Exception:\n","        from whisperx import DiarizationPipeline\n","    diar = DiarizationPipeline(device=DEVICE, use_auth_token=(CFG.HUGGINGFACE_TOKEN or None))\n","    diar_out = diar(AUDIO_PATH)\n","else:\n","    diar_out = {\"segments\": []}\n","\n","# Alignment\n","if CFG.USE_ALIGNMENT:\n","    try:\n","        align_model, metadata = whisperx.load_align_model(language_code=asr[\"language\"], device=DEVICE)\n","        asr_aligned = whisperx.align(asr[\"segments\"], align_model, metadata, AUDIO_PATH, DEVICE)\n","    except AttributeError:\n","        from whisperx.alignment import load_align_model, align\n","        align_model, metadata = load_align_model(language_code=asr[\"language\"], device=DEVICE)\n","        asr_aligned = align(asr[\"segments\"], align_model, metadata, AUDIO_PATH, DEVICE)\n","else:\n","    asr_aligned = {\"segments\": asr[\"segments\"]}\n","\n","# Assign speakers\n","asr_spk = whisperx.assign_word_speakers(diar_out, asr_aligned)\n","\n","rows = [{\n","    \"video_id\": os.path.basename(AUDIO_PATH),\n","    \"t_start\": s.get(\"start\"),\n","    \"t_end\": s.get(\"end\"),\n","    \"speaker\": s.get(\"speaker\", \"SPEAKER_00\"),\n","    \"text\": (s.get(\"text\") or \"\").strip(),\n","} for s in asr_spk[\"segments\"]]\n","\n","df = pd.DataFrame(rows)\n","df.to_parquet(\"UTTERANCES.parquet\", index=False)\n","print(\"‚úÖ wrote UTTERANCES.parquet with\", len(df), \"rows\")\n","try:\n","    display(df.head(10))\n","except Exception:\n","    print(df.head(10).to_string(index=False))\n"]},{"cell_type":"code","execution_count":null,"id":"Snh2mw2bYK3F","metadata":{"id":"Snh2mw2bYK3F"},"outputs":[],"source":["# Smoke test\n","import os, pandas as pd\n","assert os.path.exists(\"UTTERANCES.parquet\"), \"Missing UTTERANCES.parquet\"\n","df = pd.read_parquet(\"UTTERANCES.parquet\")\n","required = {\"video_id\",\"t_start\",\"t_end\",\"speaker\",\"text\"}\n","assert required.issubset(df.columns), f\"Missing cols: {required - set(df.columns)}\"\n","assert len(df) > 0, \"No rows produced\"\n","print(\"‚úÖ ASR+Diarize smoke test passed. Rows:\", len(df))\n"]},{"cell_type":"code","execution_count":null,"id":"eSpWsoYAYK3F","metadata":{"id":"eSpWsoYAYK3F"},"outputs":[],"source":["# Snapshot\n","import json, time, os, subprocess, sys, torch, pandas as pd\n","snap = {\n","  \"ts\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n","  \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n","  \"asr_model\": \"small.en\",\n","  \"pip_freeze\": subprocess.check_output([\"pip\",\"freeze\"], text=True).splitlines()[:150],\n","}\n","os.makedirs(\"snapshots\", exist_ok=True)\n","import time as _t\n","p = f\"snapshots/ASR_DIA_SNAPSHOT_{int(_t.time())}.json\"\n","with open(p,\"w\") as f: json.dump(snap,f,indent=2)\n","print(\"üì∏ Saved:\", p)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":5}